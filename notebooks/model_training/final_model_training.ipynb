{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Final Model Training and Registery - Trainin on entire dataset using the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages on databricks\n",
    "working_remotely = 'spark' not in locals()\n",
    "print(f'working_remotely: {working_remotely}')\n",
    "\n",
    "if not working_remotely:\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', '-r', 'requirements.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f77e111-48f9-49bf-b340-73f345f8832b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_contour,\n",
    "    plot_param_importances,\n",
    ")\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import TFTModel\n",
    "from darts.metrics import mape, smape, mae, ope, rmse\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, IceCreamHeaterDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression, GumbelLikelihood, GaussianLikelihood\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "from darts.models import (\n",
    "    TFTModel,\n",
    "    LinearRegressionModel,\n",
    "    LightGBMModel,\n",
    "    RNNModel,\n",
    "    TCNModel,\n",
    "    TransformerModel,\n",
    "    NBEATSModel,\n",
    "    BlockRNNModel,\n",
    "    VARIMA,\n",
    ")\n",
    "\n",
    "from torchmetrics import MeanAbsolutePercentageError, MeanAbsoluteError\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# define log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a2c409-0923-4993-8a52-ed885e9b5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb2dbed-1d3e-4e19-87be-8151e566cda9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b260bab5-6e24-4f6c-819b-96867ff90302",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# custom modules\n",
    "import src.data_engineering.data_engineering as de\n",
    "from src.utils import plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769fe038-7a50-4b8b-8eb0-372244b48601",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark connection and tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a355de-7660-4bb3-8789-87948800c08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if working_remotely:\n",
    "    # not on databricks\n",
    "    # load env variables for tracking uri\n",
    "    # and create spark connection\n",
    "    from databricks.connect import DatabricksSession\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    spark = DatabricksSession.builder.remote(\n",
    "      host       = f\"{os.environ['DATABRICKS_HOST']}\",\n",
    "      token      = os.environ['DATABRICKS_TOKEN'],\n",
    "      cluster_id = os.environ['CLUSTER_ID']\n",
    "    ).getOrCreate()\n",
    "    \n",
    "else:\n",
    "    # we're on databricks\n",
    "    mlflow.set_tracking_uri(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b8a7e4-a32c-483e-b417-28bce69c846f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# get data from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "facd06ea-42fc-4b00-a130-6953134b85ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "refresh_data = (\n",
    "    'lmp_df.parquet' not in os.listdir() or\n",
    "    'mtlf_df.parquet' not in os.listdir() or\n",
    "    'mtrf_df.parquet' not in os.listdir()\n",
    ")\n",
    "refresh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8edd14c-9054-443d-af7d-ca757e85e10d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## lmp\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM sandbox_data_science.spp_weis.lmp_hourly'\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('lmp_df.parquet')\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0e6d66-a7b0-4525-a93c-952cee98ae33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## mtrf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM sandbox_data_science.spp_weis.mtrf'\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('mtrf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a953c5a1-6b78-499f-aa5e-e0742d4eaa8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## mtlf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM sandbox_data_science.spp_weis.mtlf'\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('mtlf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe84437-f726-4282-a522-a8e69d598674",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80032709-ef03-4d4e-8463-9cf8acd27b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "price_df = pd.read_parquet('lmp_df.parquet')\n",
    "mtlf_df = pd.read_parquet('mtlf_df.parquet').sort_values('GMTIntervalEnd')\n",
    "mtrf_df = pd.read_parquet('mtrf_df.parquet').sort_values('GMTIntervalEnd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74464c68-0b26-4511-877d-3aa4d50d0f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmp\n",
    "psco_lmp_df, list_nodes_name, psco_price_df_long = de.get_psco_price_df(price_df)\n",
    "lmp_series = de.create_psco_price_series(psco_lmp_df, list_nodes_name)\n",
    "\n",
    "# mtlf series\n",
    "mtlf_series, avg_act_series = de.create_mtlf_series(mtlf_df)\n",
    "# mtlf_series, avg_act_series = de.create_mtlf_lmp_series(mtlf_df, psco_lmp_df, list_nodes_name)\n",
    "\n",
    "# mtrf series\n",
    "mtrf_ratio_df = de.add_enrgy_ratio_to_mtrf(mtlf_df, mtrf_df)\n",
    "mtrf_ratio_df = de.add_enrgy_ratio_diff_to_mtrf(mtrf_ratio_df)\n",
    "mtrf_series = de.create_mtrf_series(mtrf_ratio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76693215-b2d8-48eb-adc9-02ba90de91a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_nodes_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9970e50-790b-4966-87ba-81d297bf3125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preprocess series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3671d03a-bb10-4e3e-bdc3-5045182843ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "start_time = pd.Timestamp('2023-04-02 00:00:00')\n",
    "### TIME CHANGE ########################################################\n",
    "input_chunk_length = 24*14\n",
    "forecast_horizon = 24*7\n",
    "# training_cutoff = pd.Timestamp(\"2023-06-01 06:00:00\")\n",
    "## first we need to modify the starting time in lpm series and then calculate the training cutoff\n",
    "lmp_series_starts = de.lmp_series_start_time(lmp_series, start_time)\n",
    "training_cutoff = de.get_train_cutoff(lmp_series_starts, forecast_horizon)\n",
    "print(f'training_cutoff: {training_cutoff}')\n",
    "########################################################################\n",
    "# lmp_series_horizon_dropped = lmp_series_drop_horizon(lmp_series, start_time, forecast_horizon)\n",
    "lmp_series_train, lmp_series_val, lmp_series_all = de.get_lmp_train_test_series(lmp_series_starts, training_cutoff, forecast_horizon, input_chunk_length)\n",
    "(lmp_series_train_transformed, \n",
    " lmp_series_val_transformed, \n",
    " lmp_series_transformed,\n",
    " lmp_scaler) = de.scale_series(lmp_series_train, lmp_series_val, lmp_series_all, global_fit=True)\n",
    "scalers['series'] = lmp_scaler\n",
    "\n",
    "print(f'train start: {lmp_series_train.start_time()}')\n",
    "print(f'train end: {lmp_series_train.end_time()}')\n",
    "print(f'val start: {lmp_series_val.start_time()}')\n",
    "print(f'val end: {lmp_series_val.end_time()}')\n",
    "\n",
    "\n",
    "mtlf_series_train, mtlf_series_val, mtlf_series = de.get_mtlf_train_test_series(\n",
    "    mtlf_series, start_time, training_cutoff, forecast_horizon, input_chunk_length\n",
    "    )\n",
    "# (mtlf_series_train_transformed, \n",
    "#  mtlf_series_val_transformed, \n",
    "#  mtlf_series_transformed, \n",
    "#  mtlf_scaler) = de.scale_series(mtlf_series_train, mtlf_series_val, mtlf_series)\n",
    "# scalers['mtlf'] = mtlf_scaler\n",
    "\n",
    "# print(f'train start: {mtlf_series_train.start_time()}')\n",
    "# print(f'train end: {mtlf_series_train.end_time()}')\n",
    "# print(f'val start: {mtlf_series_val.start_time()}')\n",
    "# print(f'val end: {mtlf_series_val.end_time()}')\n",
    "\n",
    "\n",
    "avg_act_series_train, avg_act_series_val, avg_act_series = de.get_avg_act_train_test_series(avg_act_series, start_time, training_cutoff)\n",
    "(avg_act_series_train_transformed, \n",
    " avg_act_series_val_transformed, \n",
    " avg_act_series_transformed,\n",
    " past_scaler) = de.scale_series(avg_act_series_train, avg_act_series_val, avg_act_series)\n",
    "scalers['pc'] = past_scaler\n",
    "\n",
    "print(f'train start: {avg_act_series_train.start_time()}')\n",
    "print(f'train end: {avg_act_series_train.end_time()}')\n",
    "print(f'val start: {avg_act_series_val.start_time()}')\n",
    "print(f'val end: {avg_act_series_val.end_time()}')\n",
    "\n",
    "\n",
    "mtrf_series_train, mtrf_series_val, mtrf_series = de.get_mtrf_train_test_series(\n",
    "    mtrf_series, start_time, training_cutoff, forecast_horizon, input_chunk_length\n",
    "    )\n",
    "# mtrf_series_train_transformed, mtrf_series_val_transformed, mtrf_series_transformed = de.scale_mtrf_series(mtrf_series_train, mtrf_series_val, mtrf_series)\n",
    "\n",
    "# print(f'train start: {mtrf_series_train.start_time()}')\n",
    "# print(f'train end: {mtrf_series_train.end_time()}')\n",
    "# print(f'val start: {mtrf_series_val.start_time()}')\n",
    "# print(f'val end: {mtrf_series_val.end_time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7197b-a709-4c71-9576-67445bfce11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "past_cov_train = avg_act_series_train_transformed\n",
    "past_cov_val = avg_act_series_val_transformed\n",
    "past_cov = avg_act_series_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7068e6c7-8c21-4d41-bbd0-974c15bffa9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future training covariates\n",
    "future_covariates_train = concatenate([mtlf_series_train, mtrf_series_train], axis=1)\n",
    "future_covariates_train.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e761aa-008f-4772-b8b5-18093014eec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future validation covariates\n",
    "end_time = mtlf_series_val.end_time() + pd.Timedelta('1H')\n",
    "mtrf_series_val_end_droped = mtrf_series_val.drop_after(end_time)\n",
    "\n",
    "future_covariates_val = concatenate([mtlf_series_val, mtrf_series_val_end_droped], axis=1)\n",
    "future_covariates_val.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2187efbc-bfaa-4d0e-a9ee-3dcde69e5076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the entire covariate series\n",
    "mtrf_series_end_droped = mtrf_series.drop_after(end_time)\n",
    "\n",
    "future_covariates = concatenate([mtlf_series, mtrf_series_end_droped], axis=1)\n",
    "future_covariates.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52eb663-bc36-45d8-97d9-0b3cbb722e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(future_covariates_train_transformed, \n",
    " future_covariates_val_transformed, \n",
    " future_covariates_transformed, \n",
    " future_scaler) = de.scale_series(future_covariates_train, future_covariates_val, future_covariates)\n",
    "scalers['fc'] = future_scaler\n",
    "\n",
    "print(f'train start: {future_covariates_train.start_time()}')\n",
    "print(f'train end: {future_covariates_train.end_time()}')\n",
    "print(f'val start: {future_covariates_val.start_time()}')\n",
    "print(f'val end: {future_covariates_val.end_time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6e78b9-1b3f-44fd-b773-09854dcf3d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lmp_train_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_train_all.append(lmp_series_train_transformed[list_nodes_name[i]])\n",
    "\n",
    "lmp_val_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_val_all.append(lmp_series_val_transformed[list_nodes_name[i]])\n",
    "\n",
    "lmp_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_all.append(lmp_series_transformed[list_nodes_name[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa23eb3-e67f-47ef-b6be-cea9e98a019d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4fab0c-ea6a-4681-8639-bd7366f86ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Final Model Training with MLFlow Experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a271783b-c518-4cf7-8b06-124d6641d7d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up MLFlow experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b83a79f-1947-4670-a149-0e0849c08b54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "user_name = 'Faezeh.Ebrahimi@xcelenergy.com'\n",
    "\n",
    "# Initialize hyperparameter tuning experiment\n",
    "experiment_name_tune = 'spp_weis_hyperparameter_tuning'\n",
    "experiment_path_tune = f\"/Users/{user_name}/{experiment_name_tune}\"\n",
    "mlflow.set_experiment(experiment_path_tune)\n",
    "experiment_id_tune = client.get_experiment_by_name(experiment_path_tune).experiment_id\n",
    "\n",
    "# Initialize final model training and loging experiment\n",
    "experiment_name_final = 'spp_weis_final_model'\n",
    "experiment_path_final = f\"/Users/{user_name}/{experiment_name_final}\"\n",
    "mlflow.set_experiment(experiment_path_final)\n",
    "experiment_id_final = client.get_experiment_by_name(experiment_path_final).experiment_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select training series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select series for training\n",
    "num_training_series = min(10, len(lmp_train_all))\n",
    "training_series = [lmp_train_all[i] for i in range(num_training_series)]\n",
    "val_series = [lmp_val_all[i] for i in range(num_training_series)]\n",
    "# for final training on all data\n",
    "all_series = [lmp_all[i] for i in range(num_training_series)]\n",
    "# list of series to make a prediction for\n",
    "list_nodes_to_predict = [list_nodes_name[i] for i in range(num_training_series)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 200\n",
    "figsize = (16, 5)\n",
    "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
    "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
    "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\"\n",
    "\n",
    "# default quantiles for QuantileRegression\n",
    "quantiles = [\n",
    "        0.01,\n",
    "        0.05,\n",
    "        0.1,\n",
    "        0.15,\n",
    "        0.2,\n",
    "        0.25,\n",
    "        0.3,\n",
    "        0.4,\n",
    "        0.5,\n",
    "        0.6,\n",
    "        0.7,\n",
    "        0.75,\n",
    "        0.8,\n",
    "        0.85,\n",
    "        0.9,\n",
    "        0.95,\n",
    "        0.99,\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5865a36d-7609-4b19-890a-a0d02f9419f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retreive the hyperparameter tuning experiment_id, the best run associated with it, and the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7abcc64-3663-476e-a5de-9e51d84fa279",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tuning_runs = mlflow.search_runs(experiment_ids= experiment_id_tune ,order_by=[\"metrics.trial_mae ASC\"])\n",
    "\n",
    "best_run_tune = client.get_run(tuning_runs.iloc[0].run_id)\n",
    "best_params_tune = best_run_tune.data.params\n",
    "best_params_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290c26a9-71e6-4d7d-ac24-e9b0d8dd7575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLFlow saves parameters keys and values in string format. we need to convert the values back to their original type.\n",
    "def convert_params(key,value):\n",
    "    if key == 'likelihood':\n",
    "        return QuantileRegression(quantiles=quantiles)\n",
    "    else:\n",
    "        return eval(value)\n",
    "\n",
    "best_params_converted = {k: convert_params(k, v) for k, v in best_params_tune.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3d777f-7856-4e37-9241-afc5d37a568e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5458dad5-a9ca-46fc-81f8-59e6ab2f13f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af418dd6-5894-4ee3-a479-ac3417a4474f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Custom Class for Darts TFT Global Model for model loading and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca5736f-87f0-4345-bec3-f1f3d1fa5e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import time\n",
    "\n",
    "\n",
    "class DartsTFTGlobalModel(mlflow.pyfunc.PythonModel):\n",
    "    # def __init__(self, tft_model):\n",
    "    #     self.model = tft_model\n",
    "\n",
    "    # def load_context(self, context):\n",
    "    #     self.model.load(context.artifacts[\"model\"])\n",
    "\n",
    "    def load_context(self, context):\n",
    "        from darts.models import TFTModel\n",
    "        import pickle\n",
    "        # print(f'context.artifacts: {context.artifacts}')\n",
    "        # device = 0 if torch.cuda.is_available() else -1\n",
    "        self.model = TFTModel.load(context.artifacts[\"model\"], map_location=torch.device('cpu'))\n",
    "        with open(context.artifacts[\"scalers\"], 'rb') as handle:\n",
    "            self.scalers = pickle.load(handle)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.model.__repr__()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.model.__str__()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Custom predict function for TFTModel.\n",
    "        Args:\n",
    "            model_input: pd.DataFrame. Containes the unscaled serie to make prediction for,\n",
    "                         future covariate series, and past covariate series as columns of a dataframe.\n",
    "        Returns:\n",
    "            prediction: json-formatted time series in original scale.\n",
    "        \"\"\"\n",
    "        # \".from_json() returns a float64 dtype\"\n",
    "        series = TimeSeries.from_json(model_input.iloc[:,0][0]).astype(np.float32) \n",
    "        past_covariates = TimeSeries.from_json(model_input.iloc[:,1][0]).astype(np.float32)\n",
    "        future_covariates = TimeSeries.from_json(model_input.iloc[:,2][0]).astype(np.float32)\n",
    "        forecast_horizon = model_input.iloc[:,3].item()\n",
    "        num_samples = model_input.iloc[:,4].item()\n",
    "\n",
    "        # scale time series\n",
    "        \n",
    "        series_scaled = TimeSeries.from_dataframe(\n",
    "            series.pd_dataframe()/self.scalers['series']\n",
    "            )\n",
    "\n",
    "        past_covariates_scaled = TimeSeries.from_dataframe(\n",
    "            past_covariates.pd_dataframe()/self.scalers['pc']\n",
    "            )\n",
    "        \n",
    "        future_covariates_scaled = TimeSeries.from_dataframe(\n",
    "            future_covariates.pd_dataframe()/self.scalers['fc']\n",
    "            )\n",
    "\n",
    "        pred_series = self.model.predict(\n",
    "                series=series_scaled,\n",
    "                past_covariates=past_covariates_scaled,\n",
    "                future_covariates=future_covariates_scaled,\n",
    "                n=forecast_horizon,\n",
    "                num_samples=num_samples\n",
    "            )\n",
    "        \n",
    "        pred_series = TimeSeries.from_dataframe(\n",
    "            pred_series.pd_dataframe()*self.scalers['series']\n",
    "            )\n",
    "\n",
    "        return TimeSeries.to_json(pred_series)\n",
    "    \n",
    "#Changed the output format pf prediction function due to the folowing exception error:\n",
    "#Exception: Request failed with status 400, {\"error_code\": \"BAD_REQUEST\", \"message\": \"Encountered an unexpected error while converting model response to JSON.Error 'Object of type TimeSeries is not JSON serializable'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f046952a-03d9-48d4-80d8-0f4864033daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an input example to infer signature\n",
    "node_series = lmp_series[list_nodes_name[1]]\n",
    "past_cov_series = avg_act_series\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "data = {\n",
    "    'series': [node_series.to_json()],\n",
    "    'past_covariates': [past_cov_series.to_json()],\n",
    "    'future_covariates': [future_cov_series.to_json()],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': 200\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "ouput_example = 'the endpoint return json as a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b588a7dc-bf97-4bed-b85c-a02ef12ad35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this signature will be logged with the model\n",
    "# registered models must have signatures\n",
    "from mlflow.models import infer_signature\n",
    "darts_tft_signature = infer_signature(df, ouput_example)\n",
    "darts_tft_signature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ccb07b-e244-4ad0-b1b5-fa9d21020126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "run_backtest = True\n",
    "artifact_path = \"/dbfs/FileStore/models/global_tft_model/final_model\"\n",
    "\n",
    "with mlflow.start_run(run_name='DartsGlobalModel_TFT_final_model', experiment_id=experiment_id_final) as run:\n",
    "    print(f'run: {run}')\n",
    "\n",
    "    tft_best_params = {\n",
    "        \"input_chunk_length\": best_params_converted[\"input_chunk_length\"],\n",
    "        \"output_chunk_length\": best_params_converted[\"output_chunk_length\"],\n",
    "        \"hidden_size\": best_params_converted[\"hidden_size\"],\n",
    "        \"lstm_layers\": best_params_converted[\"lstm_layers\"],\n",
    "        \"num_attention_heads\": best_params_converted[\"num_attention_heads\"],\n",
    "        \"dropout\": best_params_converted[\"dropout\"],\n",
    "        \"batch_size\": best_params_converted[\"batch_size\"],\n",
    "        \"n_epochs\": best_params_converted[\"n_epochs\"],\n",
    "        \"add_encoders\": encoders,\n",
    "        \"likelihood\": QuantileRegression(quantiles=quantiles), \n",
    "        \"optimizer_kwargs\": best_params_converted[\"optimizer_kwargs\"],\n",
    "        \"random_state\": 42,\n",
    "        \"torch_metrics\": torch_metrics,\n",
    "    }\n",
    "    \n",
    "    past_covariates = [past_cov for i in range(num_training_series)]\n",
    "    future_covariates=[future_covariates_transformed for i in range(num_training_series)]\n",
    "\n",
    "    tft_model = TFTModel(**tft_best_params)\n",
    "   \n",
    "    tft_model.fit(\n",
    "        series=training_series,\n",
    "        val_series=val_series,\n",
    "        past_covariates=past_covariates,\n",
    "        val_past_covariates=past_covariates,\n",
    "        future_covariates=future_covariates,\n",
    "        val_future_covariates=future_covariates,\n",
    "        verbose=True)\n",
    "\n",
    "\n",
    "    # log parameters for the run\n",
    "    # need to add accuracy results here...\n",
    "    final_train_val_only = False\n",
    "    refit = True\n",
    "    epochs_trained = tft_model.epochs_trained\n",
    "    n_epochs_final = 3 \n",
    "\n",
    "    params = tft_best_params\n",
    "    params[\"refit\"] = refit\n",
    "    params[\"final_train_val_only\"] = final_train_val_only\n",
    "    params[\"epochs_trained\"] = epochs_trained\n",
    "    params[\"num_training_series\"] = num_training_series\n",
    "    params[\"forecast_horizon\"] = tft_model.output_chunk_length\n",
    "    params[\"n_epochs_final\"] = n_epochs_final\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # backtesting takes a moment and generates a lot of output\n",
    "    # we can turn it off for testing\n",
    "    if run_backtest:\n",
    "        # back test on validation data\n",
    "        acc = tft_model.backtest(\n",
    "            series=val_series,\n",
    "            # series=all_series,\n",
    "            past_covariates=past_covariates,\n",
    "            future_covariates=future_covariates,\n",
    "            retrain=False,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            stride=25,\n",
    "            metric=[mae, rmse],\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        acc_df = pd.DataFrame(\n",
    "            np.mean(acc, axis=0).reshape(1,-1),\n",
    "            columns=['mae', 'rmse']\n",
    "        )\n",
    "\n",
    "        # log metrics\n",
    "        metrics['mae'] = acc_df.mae[0]\n",
    "        metrics['rmse'] = acc_df.rmse[0]\n",
    "\n",
    "\n",
    "    # finish training on entire data set before logging model\n",
    "    if final_train_val_only:\n",
    "        final_train_series = val_series\n",
    "    else:\n",
    "        # for final training on all data\n",
    "        final_train_series = all_series\n",
    "\n",
    "    if refit:\n",
    "        log.info('final training')\n",
    "        tft_model.reset_model()\n",
    "        tft_model.fit(\n",
    "                series=final_train_series,\n",
    "                past_covariates=past_covariates,\n",
    "                future_covariates=future_covariates,\n",
    "                verbose=True,\n",
    "                epochs=n_epochs_final, # continue training\n",
    "                )\n",
    "    \n",
    "    # final model back test on validation data\n",
    "    acc = tft_model.backtest(\n",
    "            series=val_series,\n",
    "            past_covariates=past_covariates,\n",
    "            future_covariates=future_covariates,\n",
    "            retrain=False,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            stride=25,\n",
    "            metric=[mae, rmse],\n",
    "            verbose=False,\n",
    "        )\n",
    "    \n",
    "    acc_df = pd.DataFrame(\n",
    "        np.mean(acc, axis=0).reshape(1,-1),\n",
    "        columns=['mae', 'rmse']\n",
    "    )\n",
    "\n",
    "    # log metrics\n",
    "    metrics['mae_final'] = acc_df.mae[0]\n",
    "    metrics['rmse_final'] = acc_df.rmse[0]\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "\n",
    "    # set up path to save model\n",
    "    model_name = \"tft_model\"\n",
    "    model_path = '/'.join([artifact_path, model_name])\n",
    "\n",
    "    shutil.rmtree(artifact_path, ignore_errors=True)\n",
    "    os.makedirs(artifact_path)\n",
    "\n",
    "    # log params\n",
    "    params['model_name'] = model_name\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # save tft model files (tft_model, tft_model.ckpt) \n",
    "    # and load them to artifacts when logging the model\n",
    "    tft_model.save(model_path)\n",
    "\n",
    "    scaler_name = 'scalers.pkl'\n",
    "    scaler_path = '/'.join([artifact_path, scaler_name])\n",
    "    with open(scaler_path, 'wb') as handle:\n",
    "        pickle.dump(scalers, handle)\n",
    "\n",
    "\n",
    "    # map model artififacts in dictionary\n",
    "    artifacts = {\n",
    "        'model': model_path,\n",
    "        'model.ckpt': model_path+'.ckpt',\n",
    "        'scalers': scaler_path,\n",
    "    }\n",
    "\n",
    "    # log model \n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path='GlobalForecasting',\n",
    "        code_path=[ 'notebooks/model_training/darts_tft_wrapper.py'],\n",
    "        signature=darts_tft_signature,\n",
    "        artifacts=artifacts,\n",
    "        # model will get loaded from artifacts, we don't need instantiate with one\n",
    "        python_model=DartsTFTGlobalModel(), \n",
    "        pip_requirements=[\"-r notebooks/model_training/requirements.txt\"],\n",
    "    )\n",
    "\n",
    "    # # log model darts_tft_signature\n",
    "    # mlflow.pyfunc.log_model(\n",
    "    #     artifact_path='GlobalForecasting-final-model',\n",
    "    #     signature=darts_tft_signature,\n",
    "    #     artifacts=artifacts,\n",
    "    #     # model will get loaded from artifacts, we don't need instantiate with one\n",
    "    #     python_model=DartsTFTGlobalModel(), \n",
    "    #     pip_requirements=\"/dbfs/FileStore/required_packages/requirements.txt\"\n",
    "    # ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354f8671-bef8-495b-9677-0b900622488c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbda6f40-48c9-4dbd-94a3-dbeb54a014e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Registery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "371f3960-c05d-4722-8bf1-4698ec79a4db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8a9662-8e19-4342-9ed3-e73f6a5ef934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load the model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c27302-cf3f-41d6-9cad-167182cfbc92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mtrf_series_end_droped = mtrf_series.drop_after(end_time)\n",
    "future_covariates = concatenate([mtlf_series, mtrf_series_end_droped], axis=1)\n",
    "future_covariates.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d91b57-f9ca-4cb7-b4fd-fb3e6aa51eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_series = lmp_series[list_nodes_name[1]]\n",
    "past_cov_series = avg_act_series\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "data = {\n",
    "    'series': [node_series.to_json()],\n",
    "    'past_covariates': [past_cov_series.to_json()],\n",
    "    'future_covariates': [future_cov_series.to_json()],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': 200\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a767725d-e5e2-4039-9fd3-16e47ddbb33c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# Create an experiment with a name that is unique and case sensitive.\n",
    "client = MlflowClient()\n",
    "\n",
    "exp = client.get_experiment_by_name(experiment_path)\n",
    "exp.experiment_id\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids = exp.experiment_id,\n",
    "    order_by=['param.mae']\n",
    "    )\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3266744f-a20f-4f4b-be5a-3d2602623270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run.to_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e657ef23-d71c-4331-ab8d-447b7fcfa53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame([r.to_dictionary()['info'] for r in runs])\n",
    "metrics_df = pd.DataFrame([r.to_dictionary()['data']['metrics'] for r in runs])\n",
    "params_df = pd.DataFrame([r.to_dictionary()['data']['params'] for r in runs])\n",
    "model_log_df = pd.concat([info_df, metrics_df, params_df], axis=1)\n",
    "model_log_df.sort_values('mae', ascending=True, inplace=True)\n",
    "model_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc83b57-3fae-4e14-b954-9fe0b92b2e45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: need to update so only logs model if it's not already logged\n",
    "best_run_id = model_log_df.run_id.iloc[0]\n",
    "best_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84af96a4-5ab5-4068-ad12-7300a5078e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# logged_model = 'runs:/2eeff386c66f45d98c312a63cbd91557/GlobalForecasting'\n",
    "logged_model = f'runs:/{best_run_id}/GlobalForecasting'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "pred = loaded_model.predict(df)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618e67e5-651a-429b-b270-efd82f7343fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TimeSeries.from_json(pred).mean(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd26d61d-f3d7-4458-935c-a92a05190c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d0154e-e2c0-445b-a21b-222f1cf446c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb999a34-7dc8-404d-9313-6d1761e5b782",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: register model only if it hasn't been logged yet\n",
    "catalog = \"sandbox_data_science\"\n",
    "schema = \"spp_weis\"\n",
    "model_name = \"GlobalForecasting\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.register_model(\n",
    "    model_uri=logged_model,\n",
    "    name=f\"{catalog}.{schema}.{model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962ee461-f112-4e3f-af41-f7433abe6c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for programtic deployments see\n",
    "# https://docs.databricks.com/_extras/notebooks/source/machine-learning/model-serving-endpoint-python.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5723e747-9b60-47d1-b85a-6f5ae418af06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76cb082-c447-4c28-956b-30678e1d3b83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prediction using API endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e841b4-aa93-45d4-bbc1-ef2756dbeb0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_series = lmp_series[list_nodes_name[1]]\n",
    "past_cov_series = avg_act_series\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "data = {\n",
    "    'series': [node_series.to_json()],\n",
    "    'past_covariates': [past_cov_series.to_json()],\n",
    "    'future_covariates': [future_cov_series.to_json()],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': 200\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b35ed2-227c-4f22-b4c6-116f66d818a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "  return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "  url = 'https://dbc-beada314-1494.cloud.databricks.com/serving-endpoints/spp_weis/invocations'\n",
    "  api_token = os.environ['DATABRICKS_TOKEN']\n",
    "  headers = {'Authorization': f'Bearer {api_token}', 'Content-Type': 'application/json'}\n",
    "  ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "  data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "  response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "  if response.status_code != 200:\n",
    "    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592b4145-2290-43c6-be0d-21f10f72e8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_pred = score_model(df)\n",
    "# endpoint_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f5a6ad-eb16-4bfb-a495-72525185db4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = TimeSeries.from_json(endpoint_pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64588d01-a870-4a52-98f5-2787d42ab9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds.mean(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc14ad4-f4c3-4fcd-b060-a0891a16a7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_quantile_df(preds):\n",
    "\n",
    "    # get dataframe from preds TimeSeries\n",
    "    plot_df = (\n",
    "        preds.pd_dataframe()\n",
    "        .reset_index()\n",
    "        .melt(id_vars='time')\n",
    "        .rename(columns={'component':'node'})\n",
    "    )\n",
    "\n",
    "    # remove sample numbers\n",
    "    plot_df.node = ['_'.join(n.split('_')[:-1]) for n in plot_df.node]\n",
    "\n",
    "    # get quanitles\n",
    "    q_df = plot_df.groupby(['time', 'node']).quantile([0.1, 0.5, 0.9])\n",
    "\n",
    "    # create columns from quantiles\n",
    "    q_pivot = q_df.reset_index().pivot(columns='level_2', index=['time', 'node'])\n",
    "\n",
    "    # level from columns after pivot\n",
    "    q_pivot.columns = q_pivot.columns.droplevel()\n",
    "\n",
    "    # remove index level name\n",
    "    q_pivot.columns.name = None\n",
    "    \n",
    "    return q_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae44bfa-7065-491d-8f59-8bd3a5413b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q_df = get_quantile_df(preds)\n",
    "q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d46028-35b7-4188-b7f8-82c08ab685aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_df(preds):\n",
    "    plot_df = (\n",
    "        preds.pd_dataframe()\n",
    "        .reset_index()\n",
    "        .melt(id_vars='time')\n",
    "        .rename(columns={'component':'node'})\n",
    "    )\n",
    "\n",
    "    # remove sample numbers\n",
    "    plot_df.node = ['_'.join(n.split('_')[:-1]) for n in plot_df.node]\n",
    "\n",
    "    # get quanitles\n",
    "    mean_df = plot_df.groupby(['time', 'node']).mean()\n",
    "    mean_df.rename(columns={'value':'mean'}, inplace=True)\n",
    "    return mean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72892c3c-f8f4-4877-b6c1-aca4b3b95bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_df = get_mean_df(preds).merge(\n",
    "    get_quantile_df(preds),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d9fdc9-65ab-4082-818d-b230c231f3e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb737393-3f08-4e25-9776-f253395bfb0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 816248818070759,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "globalmodel_hyperparameter_tuning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
