{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ef0bdb-45a2-4734-821b-c6dece9ab18c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# pip install darts\n",
    "# conda install -c conda-forge -c pytorch u8darts-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f77e111-48f9-49bf-b340-73f345f8832b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import TFTModel\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, IceCreamHeaterDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression, GumbelLikelihood, GaussianLikelihood\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "from darts.models import (\n",
    "    TFTModel,\n",
    "    LinearRegressionModel,\n",
    "    LightGBMModel,\n",
    "    RNNModel,\n",
    "    TCNModel,\n",
    "    TransformerModel,\n",
    "    NBEATSModel,\n",
    "    BlockRNNModel,\n",
    "    VARIMA,\n",
    ")\n",
    "\n",
    "from darts.metrics.metrics import mape, smape, ope\n",
    "\n",
    "\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# define log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfe87952-6a2e-4ba5-abd6-e7a2469b1343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# modeling packages\n",
    "os.chdir('../..')\n",
    "\n",
    "import mlflow\n",
    "import src.modeling.darts_tft_wrapper as m\n",
    "from src.modeling.darts_tft_wrapper import DartsTFTGlobalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a2c409-0923-4993-8a52-ed885e9b5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7193060-bb06-48cf-91ba-6fa2cae7a13d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6551a455-5086-43a0-b3bf-8bbc4a58cb8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import src.data_engineering.data_engineering as de\n",
    "# import src.feature_engineering.feature_engineering as fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769fe038-7a50-4b8b-8eb0-372244b48601",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a355de-7660-4bb3-8789-87948800c08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if 'spark' not in locals():\n",
    "    from databricks.connect import DatabricksSession\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    spark = DatabricksSession.builder.remote(\n",
    "      host       = f\"https://{os.environ['DATABRICKS_HOST']}\",\n",
    "      token      = os.environ['SPP_WEIS'],\n",
    "      cluster_id = os.environ['CLUSTER_ID']\n",
    "    ).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b8a7e4-a32c-483e-b417-28bce69c846f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# get data from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "facd06ea-42fc-4b00-a130-6953134b85ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "refresh_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8edd14c-9054-443d-af7d-ca757e85e10d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## lmp\n",
    "if refresh_data:\n",
    "    query = '''\n",
    "    SELECT \n",
    "      GMTIntervalEnd_HE as GMTIntervalEnd,\n",
    "      PNODE_Name,\n",
    "      avg(LMP) as LMP_HOURLY\n",
    "    FROM sandbox_data_science.spp_weis.lmp\n",
    "    GROUP BY GMTIntervalEnd_HE, PNODE_Name\n",
    "    '''\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('lmp_df.parquet')\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0e6d66-a7b0-4525-a93c-952cee98ae33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## mtrf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM sandbox_data_science.spp_weis.mtrf'\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('mtrf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a953c5a1-6b78-499f-aa5e-e0742d4eaa8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## mtlf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM sandbox_data_science.spp_weis.mtlf'\n",
    "    res = spark.sql(query).collect()\n",
    "    df = spark.createDataFrame(res).toPandas()\n",
    "    df.to_parquet('mtlf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efe84437-f726-4282-a522-a8e69d598674",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80032709-ef03-4d4e-8463-9cf8acd27b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "price_df = pd.read_parquet('lmp_df.parquet')\n",
    "mtlf_df = pd.read_parquet('mtlf_df.parquet').sort_values('GMTIntervalEnd')\n",
    "mtrf_df = pd.read_parquet('mtrf_df.parquet').sort_values('GMTIntervalEnd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74464c68-0b26-4511-877d-3aa4d50d0f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411e8a60-0b16-47a1-88fd-94f54fece00c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# fill missing values\n",
    "########################################\n",
    "def fill_missed_values(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = df.fillna(method='ffill')\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# lmp\n",
    "########################################\n",
    "def get_psco_price_df(price_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    psco_idx = price_df[\"PNODE_Name\"].str.contains(\"PSCO\", case=False)\n",
    "    psco_price_df = price_df[psco_idx]\n",
    "    psco_price_df = psco_price_df.pivot_table(\n",
    "                            index='GMTIntervalEnd',\n",
    "                            columns='PNODE_Name',\n",
    "                            values='LMP_HOURLY',\n",
    "                            margins=False,\n",
    "                        ).reset_index()\n",
    "\n",
    "    psco_price_df.columns.name=None\n",
    "\n",
    "    ls_nodes_name = list(psco_price_df.columns[1:])\n",
    "\n",
    "    return fill_missed_values(psco_price_df), ls_nodes_name\n",
    "\n",
    "\n",
    "def create_psco_price_series(psco_price_df, node_name_ls):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    psco_price_series = TimeSeries.from_dataframe(psco_price_df, time_col='GMTIntervalEnd', value_cols=node_name_ls, fill_missing_dates=True, freq='H', fillna_value=0, static_covariates=None, hierarchy=None).astype(np.float32)\n",
    "\n",
    "    return psco_price_series\n",
    "\n",
    "\n",
    "########################################\n",
    "# mtlf\n",
    "########################################\n",
    "def create_mtlf_series(mtlf_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mtlf_df = fill_missed_values(mtlf_df)\n",
    "    mtlf_series = TimeSeries.from_dataframe(mtlf_df, time_col='GMTIntervalEnd', value_cols='MTLF', fill_missing_dates=True, freq='H', fillna_value=0, static_covariates=None, hierarchy=None).astype(np.float32)\n",
    "    avg_act_series = TimeSeries.from_dataframe(mtlf_df, time_col='GMTIntervalEnd', value_cols='Averaged_Actual', fill_missing_dates=True, freq='H', fillna_value=0, static_covariates=None, hierarchy=None).astype(np.float32)\n",
    "    \n",
    "    return mtlf_series, avg_act_series\n",
    "          \n",
    "\n",
    "########################################\n",
    "# mtrf\n",
    "########################################  \n",
    "# Add renewable/load ratio feature to mtrf dataframe\n",
    "def add_enrgy_ratio_to_mtrf(mtlf_df, mtrf_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mtrf_df = mtrf_df[['GMTIntervalEnd', 'Wind_Forecast_MW', 'Solar_Forecast_MW']].set_index('GMTIntervalEnd').asfreq('H').sort_index()\n",
    "    mtrf_df = mtrf_df.join(mtlf_df[['GMTIntervalEnd','MTLF']].set_index('GMTIntervalEnd').asfreq('H').sort_index(), on='GMTIntervalEnd', how='outer').sort_values('GMTIntervalEnd').reset_index(drop=True)\n",
    "    mtrf_df['Ratio'] = (mtrf_df['Wind_Forecast_MW'] + mtrf_df['Solar_Forecast_MW']) / mtrf_df['MTLF']\n",
    "    mtrf_df.drop('MTLF', axis=1, inplace=True) \n",
    "\n",
    "    return fill_missed_values(mtrf_df)\n",
    "\n",
    "def create_mtrf_series(mtrf_ratio_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mtrf_series= TimeSeries.from_dataframe(mtrf_ratio_df, time_col='GMTIntervalEnd', value_cols=['Wind_Forecast_MW','Solar_Forecast_MW', 'Ratio'], fill_missing_dates=True, freq='H', fillna_value=0, static_covariates=None, hierarchy=None).astype(np.float32)\n",
    "    \n",
    "    return mtrf_series     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47977262-0c35-41fd-b9c7-a68a3a30afa2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lmp\n",
    "psco_lmp_df, list_nodes_name = get_psco_price_df(price_df)\n",
    "lmp_series = create_psco_price_series(psco_lmp_df, list_nodes_name)\n",
    "\n",
    "# mtlf series\n",
    "mtlf_series, avg_act_series = create_mtlf_series(mtlf_df)\n",
    "\n",
    "# mtrf series\n",
    "mtrf_ratio_df = add_enrgy_ratio_to_mtrf(mtlf_df, mtrf_df)\n",
    "mtrf_series = create_mtrf_series(mtrf_ratio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9970e50-790b-4966-87ba-81d297bf3125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preprocess series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4fd9f4b-f027-44d2-a24b-d45d63a28c84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def lmp_series_drop_horizon(lmp_series, start_time, forecast_horizon):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    start_time_lmp = start_time\n",
    "    end_time_lmp = lmp_series.end_time() - pd.Timedelta(f'{forecast_horizon+1}H')\n",
    "    lmp_series = lmp_series.drop_before(start_time_lmp)\n",
    "    lmp_series = lmp_series.drop_after(end_time_lmp)\n",
    "    return lmp_series\n",
    "\n",
    "def get_train_cutoff(lmp_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #keep last 30 days (720 data points + horizon(72 hours)) of target series for validation. \n",
    "    training_cutoff = lmp_series[:-792].end_time()\n",
    "    return training_cutoff\n",
    "\n",
    "def get_lmp_train_test_series(lmp_series_drop_horizon, training_cutoff, forecast_horizon, input_chunk_length):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lmp_series_train = lmp_series_drop_horizon.drop_after(training_cutoff - pd.Timedelta(f'{forecast_horizon+1}H')) # for future covariates\n",
    "    lmp_series_val = lmp_series_drop_horizon.drop_before(training_cutoff + pd.Timedelta(f'{input_chunk_length+1}H'))\n",
    "    lmp_series_all = lmp_series_drop_horizon\n",
    "\n",
    "    return [lmp_series_train, lmp_series_val, lmp_series_all]\n",
    "\n",
    "\n",
    "def scale_lmp_series(lmp_series_train, lmp_series_val, lmp_series_all):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    transformer_lmp = Scaler()\n",
    "    lmp_series_train_transformed = transformer_lmp.fit_transform(lmp_series_train)\n",
    "    lmp_series_val_transformed = transformer_lmp.transform(lmp_series_val)\n",
    "    lmp_series_transformed = transformer_lmp.transform(lmp_series_all)\n",
    "\n",
    "    return [lmp_series_train_transformed, lmp_series_val_transformed, lmp_series_transformed]\n",
    "\n",
    "############## avg_act ##################\n",
    "# def get_avg_act_train_test_series(avg_act_series, start_time, training_cutoff, forecast_horizon, input_chunk_length):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     start_time_avg = start_time\n",
    "#     end_time_avg = avg_act_series.end_time() - pd.Timedelta(f'{forecast_horizon+1}H')\n",
    "#     avg_act_series = avg_act_series.drop_before(start_time_avg)\n",
    "#     avg_act_series = avg_act_series.drop_after(end_time_avg)\n",
    "\n",
    "\n",
    "#     avg_act_series_train = avg_act_series.drop_after(training_cutoff - pd.Timedelta(f'{forecast_horizon+1}H')) # for future covariates\n",
    "#     avg_act_series_val = avg_act_series.drop_before(training_cutoff + pd.Timedelta(f'{input_chunk_length+1}H'))\n",
    "\n",
    "#     return [avg_act_series_train, avg_act_series_val, avg_act_series]\n",
    "\n",
    "\n",
    "# def scale_avg_act_series(avg_act_series_train, avg_act_series_val, avg_act_series):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     transformer_avg_act = Scaler()\n",
    "#     avg_act_series_train_transformed = transformer_avg_act.fit_transform(avg_act_series_train)\n",
    "#     avg_act_series_val_transformed = transformer_avg_act.transform(avg_act_series_val)\n",
    "#     avg_act_series_transformed = transformer_avg_act.transform(avg_act_series)\n",
    "\n",
    "#     return [avg_act_series_train_transformed, avg_act_series_val_transformed, avg_act_series_transformed]\n",
    "\n",
    "############## mtlf #####################\n",
    "def get_mtlf_train_test_series(mtlf_series, start_time, training_cutoff):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # drop times before the starting time\n",
    "    mtlf_series = mtlf_series.drop_before(start_time)\n",
    "\n",
    "    # Split\n",
    "    mtlf_series_train = mtlf_series.drop_after(training_cutoff)\n",
    "    mtlf_series_val = mtlf_series.drop_before(training_cutoff)\n",
    "\n",
    "    return [mtlf_series_train, mtlf_series_val, mtlf_series]\n",
    "\n",
    "\n",
    "def scale_mtlf_series(mtlf_series_train, mtlf_series_val, mtlf_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    transformer_mtlf = Scaler()\n",
    "    mtlf_series_train_transformed = transformer_mtlf.fit_transform(mtlf_series_train)\n",
    "    mtlf_series_val_transformed = transformer_mtlf.transform(mtlf_series_val)\n",
    "    mtlf_series_transformed = transformer_mtlf.transform(mtlf_series)\n",
    "\n",
    "    return [mtlf_series_train_transformed, mtlf_series_val_transformed, mtlf_series_transformed]\n",
    "\n",
    "############# avg_actual #################\n",
    "def get_avg_act_train_test_series(avg_act_series, start_time, training_cutoff):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    avg_act_series = avg_act_series.drop_before(start_time)\n",
    "\n",
    "    # Split\n",
    "    avg_act_series_train = avg_act_series.drop_after(training_cutoff)\n",
    "    avg_act_series_val = avg_act_series.drop_before(training_cutoff)\n",
    "\n",
    "    return [avg_act_series_train, avg_act_series_val, avg_act_series]\n",
    "\n",
    "\n",
    "def scale_avg_act_series(avg_act_series_train, avg_act_series_val, avg_act_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    transformer_avg = Scaler()\n",
    "    avg_act_series_train_transformed = transformer_avg.fit_transform(avg_act_series_train)\n",
    "    avg_act_series_val_transformed = transformer_avg.transform(avg_act_series_val)\n",
    "    avg_act_series_transformed = transformer_avg.transform(avg_act_series)\n",
    "\n",
    "    return [avg_act_series_train_transformed, avg_act_series_val_transformed, avg_act_series_transformed]\n",
    "\n",
    "\n",
    "############## mtrf #####################\n",
    "def get_mtrf_train_test_series(mtrf_series, start_time, training_cutoff):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mtrf_series = mtrf_series.drop_before(start_time)\n",
    "\n",
    "    # Split\n",
    "    mtrf_series_train = mtrf_series.drop_after(training_cutoff)\n",
    "    mtrf_series_val = mtrf_series.drop_before(training_cutoff)\n",
    "\n",
    "    return [mtrf_series_train, mtrf_series_val, mtrf_series]\n",
    "\n",
    "\n",
    "def scale_mtrf_series(mtrf_series_train, mtrf_series_val, mtrf_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    transformer_win = Scaler()\n",
    "    wind_series_train_transformed = transformer_win.fit_transform(mtrf_series_train)\n",
    "    wind_series_val_transformed = transformer_win.transform(mtrf_series_val)\n",
    "    wind_series_transformed = transformer_win.transform(mtrf_series)\n",
    "\n",
    "    return [wind_series_train_transformed, wind_series_val_transformed, wind_series_transformed]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3671d03a-bb10-4e3e-bdc3-5045182843ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp('2023-04-02 00:00:00')\n",
    "### TIME CHANGE ########################################################\n",
    "input_chunk_length = 24*7\n",
    "forecast_horizon = 24*3\n",
    "training_cutoff = pd.Timestamp(\"2023-06-01 06:00:00\")\n",
    "########################################################################\n",
    "lmp_series = lmp_series_drop_horizon(lmp_series, start_time, forecast_horizon)\n",
    "# training_cutoff = get_train_cutoff(lmp_series_drop_horizon)\n",
    "lmp_series_train, lmp_series_val, lmp_series_all = get_lmp_train_test_series(lmp_series, training_cutoff, forecast_horizon, input_chunk_length)\n",
    "lmp_series_train_transformed, lmp_series_val_transformed, lmp_series_transformed = scale_lmp_series(lmp_series_train, lmp_series_val, lmp_series_all)\n",
    "\n",
    "print(f'train start: {lmp_series_train.start_time()}')\n",
    "print(f'train end: {lmp_series_train.end_time()}')\n",
    "print(f'val start: {lmp_series_val.start_time()}')\n",
    "print(f'val end: {lmp_series_val.end_time()}')\n",
    "\n",
    "\n",
    "mtlf_series_train, mtlf_series_val, mtlf_series = get_mtlf_train_test_series(mtlf_series, start_time, training_cutoff)\n",
    "mtlf_series_train_transformed, mtlf_series_val_transformed, mtlf_series_transformed = scale_mtlf_series(mtlf_series_train, mtlf_series_val, mtlf_series)\n",
    "\n",
    "print(f'train start: {mtlf_series_train.start_time()}')\n",
    "print(f'train end: {mtlf_series_train.end_time()}')\n",
    "print(f'val start: {mtlf_series_val.start_time()}')\n",
    "print(f'val end: {mtlf_series_val.end_time()}')\n",
    "\n",
    "\n",
    "avg_act_series_train, avg_act_series_val, avg_act_series = get_avg_act_train_test_series(avg_act_series, start_time, training_cutoff)\n",
    "avg_act_series_train_transformed, avg_act_series_val_transformed, avg_act_series_transformed = scale_avg_act_series(avg_act_series_train, avg_act_series_val, avg_act_series)\n",
    "\n",
    "print(f'train start: {avg_act_series_train.start_time()}')\n",
    "print(f'train end: {avg_act_series_train.end_time()}')\n",
    "print(f'val start: {avg_act_series_val.start_time()}')\n",
    "print(f'val end: {avg_act_series_val.end_time()}')\n",
    "\n",
    "\n",
    "\n",
    "mtrf_series_train, mtrf_series_val, mtrf_series = get_mtrf_train_test_series(mtrf_series, start_time, training_cutoff)\n",
    "mtrf_series_train_transformed, mtrf_series_val_transformed, mtrf_series_transformed = scale_mtrf_series(mtrf_series_train, mtrf_series_val, mtrf_series)\n",
    "\n",
    "print(f'train start: {mtrf_series_train.start_time()}')\n",
    "print(f'train end: {mtrf_series_train.end_time()}')\n",
    "print(f'val start: {mtrf_series_val.start_time()}')\n",
    "print(f'val end: {mtrf_series_val.end_time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7197b-a709-4c71-9576-67445bfce11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "past_cov_train = avg_act_series_train_transformed\n",
    "past_cov_val = avg_act_series_val_transformed\n",
    "past_cov = avg_act_series_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7068e6c7-8c21-4d41-bbd0-974c15bffa9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future training covariates\n",
    "future_covariates_train = concatenate([mtlf_series_train_transformed, mtrf_series_train_transformed], axis=1)\n",
    "future_covariates_train.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e761aa-008f-4772-b8b5-18093014eec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future validation covariates\n",
    "end_time = mtlf_series_val_transformed.end_time() + pd.Timedelta('1H')\n",
    "mtrf_series_val_transformed_end_droped = mtrf_series_val_transformed.drop_after(end_time)\n",
    "\n",
    "future_covariates_val = concatenate([mtlf_series_val_transformed, mtrf_series_val_transformed_end_droped], axis=1)\n",
    "future_covariates_val.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2187efbc-bfaa-4d0e-a9ee-3dcde69e5076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the entire covariate series\n",
    "mtrf_series_transformed_end_droped = mtrf_series_transformed.drop_after(end_time)\n",
    "\n",
    "future_covariates = concatenate([mtlf_series_transformed, mtrf_series_transformed_end_droped], axis=1)\n",
    "future_covariates.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6e78b9-1b3f-44fd-b773-09854dcf3d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lmp_train_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_train_all.append(lmp_series_train_transformed[list_nodes_name[i]])\n",
    "\n",
    "\n",
    "lmp_val_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_val_all.append(lmp_series_val_transformed[list_nodes_name[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4fab0c-ea6a-4681-8639-bd7366f86ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6283044-f609-4a22-901d-980f776b41da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create TFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ef3cdf-10c2-4954-a7ff-476b5b99036d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Considerations**\n",
    "- Target series: `LMP`\n",
    "- Past Covariate series: `Averaged_actula` \n",
    "- Future Covariate Series: `MTLF`, `Wind`, and `Solar`. Basically, these are mid-term load forecasts and mid-term renewables forecasts.\n",
    "- `input_chunk-length = 168 (7 days)`, `output_chunk_length = 24 hours` and **forecasting horizon could be the length of output_chunk or my validation set?**\n",
    "\n",
    "##### Note : You can pass the covariates to the model as individual series in a list, or you can stack them and pass them as one multivariate series. However, you cannot sack them if they do not have the same length.\n",
    "##### Note from Dart's TFT example: we can just provide the ***whole covariates series*** as future_covariates argument to the model; the model will slice these covariates and use only what it needs in order to train on forecasting the target train_transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22cd3f2a-25f6-4af8-85e7-b986c602a50a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit the model using covariate series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6539e838-ae87-4b39-b036-d4d3e4b95b78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 200\n",
    "\n",
    "figsize = (16, 5)\n",
    "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
    "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
    "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c085b74-b3d2-49b8-8489-5bfb9231fd02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default quantiles for QuantileRegression\n",
    "quantiles = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    0.99,\n",
    "]\n",
    "\n",
    "encoders = {\"cyclic\": {\"future\": [\"hour\", \"dayofweek\"]},\n",
    "            \"transformer\": Scaler()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2ce9a9-0900-4e9b-9289-85c32a1cf92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "torch_metrics = MeanAbsolutePercentageError()\n",
    "\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor= \"val_MeanAbsolutePercentageError\", # \"val_loss\",\n",
    "    patience=5,\n",
    "    min_delta=0.05,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "                   \"accelerator\": \"gpu\", \"devices\": [0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f0e913-817a-454a-ae68-879b2d74acf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35b5d26e-ab83-4167-b7c8-dfbacd2da13a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Log Darts TFT wrapper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e05dde3-0184-4c08-92f3-669467af732f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/68356746/changing-subdirectory-of-mlflow-artifact-store\n",
    "artifact_path = \"./model_artifacts/\"\n",
    "os.makedirs(artifact_path, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name('tft')\n",
    "    experiment_id = experiment.experiment_id\n",
    "except AttributeError:\n",
    "    experiment_id = mlflow.create_experiment('tft', artifact_location=artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d7f686-50a1-4130-ad74-ce5c7a51f72c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957227ec-2c6b-49c2-b87f-d38e585d5401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9f058d-c2db-4df3-b1de-3238a58660e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d17e46-cc84-4c18-9771-5b9d3c6e473a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# turn off pytorch lightning logs\n",
    "# https://github.com/Lightning-AI/lightning/issues/3431\n",
    "# import logging\n",
    "# import lightning\n",
    "# logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.ERROR)\n",
    "# # logging.getLogger(\"lightning.pytorch.accelerators\").setLevel(logging.WARNING)\n",
    "# logging.getLogger(\"lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94859470-d54b-4d25-a7b7-4ff4334d4400",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_backtest = True\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "    print(f'run: {run}')\n",
    "\n",
    "    lr = 1e-4\n",
    "    hidden_size = 32\n",
    "    lstm_layers = 1\n",
    "    num_attention_heads = 4\n",
    "    \n",
    "    tft_model = TFTModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=forecast_horizon,\n",
    "        hidden_size=hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        dropout=0.1,\n",
    "        batch_size=16,\n",
    "        n_epochs=2,\n",
    "        add_encoders=encoders,\n",
    "        likelihood=QuantileRegression(quantiles=quantiles),  # QuantileRegression is set per default\n",
    "        optimizer_kwargs={\"lr\": lr},\n",
    "        random_state=42,\n",
    "        torch_metrics=torch_metrics,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "    )\n",
    "    \n",
    "    tft_model.fit(\n",
    "        series=[lmp_train_all[0], lmp_train_all[1]],\n",
    "        val_series=[lmp_val_all[0], lmp_val_all[1]],\n",
    "        past_covariates=[past_cov_train for i in range(2)],\n",
    "        val_past_covariates=[past_cov_val for i in range(2)],\n",
    "        future_covariates=[future_covariates_train for i in range(2)],\n",
    "        val_future_covariates=[future_covariates_val for i in range(2)],\n",
    "        verbose=True)\n",
    "\n",
    "\n",
    "    # log parameters for the run\n",
    "    # need to add accuracy results here...\n",
    "    params = {\n",
    "        \"lr\":lr,\n",
    "        \"epochs_trained\":tft_model.epochs_trained,\n",
    "        \"num_attention_heads\":tft_model.num_attention_heads,\n",
    "        \"hidden_size\":tft_model.hidden_size,\n",
    "        \"lstm_layers\":tft_model.lstm_layers,\n",
    "        }\n",
    "\n",
    "    # backtesting takes a moment and generates a lot of output\n",
    "    # we can turn it off for testing\n",
    "    if run_backtest:\n",
    "        # back test on validation data\n",
    "        acc = tft_model.backtest(\n",
    "            series=[lmp_val_all[0], lmp_val_all[1]],\n",
    "            past_covariates=[past_cov_val for i in range(2)],\n",
    "            future_covariates=[future_covariates_val for i in range(2)],\n",
    "            retrain=False,\n",
    "            stride=6,\n",
    "            metric=[mape, smape, ope],\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        acc_df = pd.DataFrame(\n",
    "            np.mean(acc, axis=0).reshape(1,-1),\n",
    "            columns=['mape', 'smape', 'ope']\n",
    "        )\n",
    "\n",
    "        # add accuracy to params\n",
    "        params['mape'] = acc_df.mape[0]\n",
    "        params['smape'] = acc_df.smape[0]\n",
    "        params['ope'] = acc_df.ope[0]\n",
    "    \n",
    "\n",
    "    # set up path to save model\n",
    "    model_name = \"tft_model\"\n",
    "    model_path = ''.join([artifact_path, model_name])\n",
    "\n",
    "    # log params\n",
    "    params['model_name'] = model_name\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # save tft model files (tft_model, tft_model.ckpt) \n",
    "    # and load them to artifacts when logging the model\n",
    "    tft_model.save(model_path)\n",
    "\n",
    "    # map model artififacts in dictionary\n",
    "    artifacts = {\n",
    "        'model': model_path,\n",
    "        'model.ckpt': model_path+'.ckpt'\n",
    "    }\n",
    "\n",
    "    # log model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path='model_artifacts',\n",
    "        artifacts=artifacts,\n",
    "        # model will get loaded from artifacts, we don't need instantiate with one\n",
    "        python_model=DartsTFTGlobalModel(), \n",
    "    ) # registered_model_name=\"summarization-model\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e01aec0-ea81-4ca9-bc69-eb49a072cade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a053ba67-2d5a-44a1-beaa-54746dc536f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load logged model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39c25be-4818-4eeb-83ae-86a8453446e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_series = lmp_series_transformed[list_nodes_name[0]]\n",
    "past_cov_series = avg_act_series_transformed\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "data = {\n",
    "    'series': [node_series],\n",
    "    'past_covariates': [past_cov_series],\n",
    "    'future_covariates': [future_cov_series],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': num_samples\n",
    "}\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "model_input = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7595d7bc-a3ba-419a-b75f-08eaa134b880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in os.listdir(artifact_path) if isfile(join(artifact_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "561e2183-d140-43de-a93f-1f7978e50d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get paths to logged models\n",
    "model_files = []\n",
    "for root, dirs, files in os.walk(artifact_path):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        if 'model_artifacts/artifacts/tft_model' in path and '.ckpt' not in path:\n",
    "            model_files += [path]\n",
    "\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab67e2-bf23-4abb-ba5c-b7eaada9f7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model as a PyFuncModel.\n",
    "path = model_files[0]\n",
    "print(path)\n",
    "# need the directory with 'MLmodel'\n",
    "path = '/'.join(path.split('/')[:-2])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab67e2-bf23-4abb-ba5c-b7eaada9f7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded_model =  mlflow.pyfunc.load_model(path)\n",
    "type(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab67e2-bf23-4abb-ba5c-b7eaada9f7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_series = loaded_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070674aa-ce56-4793-a59b-f8a145dc09da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92812ef8-e66b-455b-aed7-2389b8e3ced8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1902709917152129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "global_model_training",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
