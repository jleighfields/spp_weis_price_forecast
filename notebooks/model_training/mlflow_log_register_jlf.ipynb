{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a70c6dc-c58f-4103-b2ee-1759b1f00232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLFlow experiment, log, and registering model\n",
    "To work remotely with Databricks several environment variable need to be set up.  These are expected to be in a `.env` file in the project root.  If you are working remotely these environment varialbles will be loaded by the `load_env()` call in the code block where the spark session is configured.  This file needs to include:\n",
    "\n",
    "* `DATABRICKS_HOST=https://dbc-beada314-1494.cloud.databricks.com`\n",
    "* `MLFLOW_TRACKING_URI=databricks`\n",
    "* `DATABRICKS_TOKEN=<your token>`\n",
    "* `MLFLOW_TRACKING_TOKEN=<your token>`\n",
    "* `CLUSTER_ID=0609-202631-imzzg29c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install darts --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f77e111-48f9-49bf-b340-73f345f8832b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from databricks import sql\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# import torch\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mape, smape, mae, ope, rmse\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, IceCreamHeaterDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression, GumbelLikelihood, GaussianLikelihood\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "from darts.models import (\n",
    "    TFTModel,\n",
    "    TiDEModel,\n",
    "    DLinearModel,\n",
    "    NLinearModel,\n",
    "    TSMixerModel\n",
    ")\n",
    "\n",
    "\n",
    "from torchmetrics import MeanAbsolutePercentageError, MeanAbsoluteError, MeanSquaredError\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# define log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TSMixerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a2c409-0923-4993-8a52-ed885e9b5c37",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7193060-bb06-48cf-91ba-6fa2cae7a13d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## will be loaded from root when deployed\n",
    "from darts_wrapper import DartsGlobalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb2dbed-1d3e-4e19-87be-8151e566cda9",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b260bab5-6e24-4f6c-819b-96867ff90302",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom modules\n",
    "import src.data_engineering.data_engineering as de\n",
    "from src.utils import plotting\n",
    "from src.utils import utils\n",
    "\n",
    "# cannot load from location other than root\n",
    "# from src.modeling.darts_tft_wrapper import DartsGlobalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769fe038-7a50-4b8b-8eb0-372244b48601",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark connection and tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a355de-7660-4bb3-8789-87948800c08c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if working_remotely:\n",
    "#     # not on databricks\n",
    "#     # load env variables for tracking uri\n",
    "#     # and create spark connection\n",
    "#     from databricks.connect import DatabricksSession\n",
    "#     from dotenv import load_dotenv\n",
    "#     load_dotenv()\n",
    "    \n",
    "#     spark = DatabricksSession.builder.remote(\n",
    "#       host       = f\"{os.environ['DATABRICKS_HOST']}\",\n",
    "#       token      = os.environ['DATABRICKS_TOKEN'],\n",
    "#       cluster_id = os.environ['CLUSTER_ID']\n",
    "#     ).getOrCreate()\n",
    "    \n",
    "# else:\n",
    "#     # we're on databricks\n",
    "#     mlflow.set_tracking_uri(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1493adc5-cd27-4d5c-b42c-eb9cb816d46e",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils.start_cluster(\n",
    "#     host       = f\"{os.environ['DATABRICKS_HOST']}\",\n",
    "#     token      = os.environ['DATABRICKS_TOKEN'],\n",
    "#     cluster_id = os.environ['CLUSTER_ID']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5170620f-6924-4901-b704-5febda9b5918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# utils.wait_for_cluster(\n",
    "#     host       = f\"{os.environ['DATABRICKS_HOST']}\",\n",
    "#     token      = os.environ['DATABRICKS_TOKEN'],\n",
    "#     cluster_id = os.environ['CLUSTER_ID']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure the cluster is started\n",
    "# utils.start_cluster(\n",
    "#     host=os.environ['DATABRICKS_HOST'], \n",
    "#     cluster_id=os.environ['CLUSTER_ID'], \n",
    "#     token=os.environ['DATABRICKS_TOKEN'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.wait_for_cluster(\n",
    "#     host=os.environ['DATABRICKS_HOST'], \n",
    "#     cluster_id=os.environ['CLUSTER_ID'], \n",
    "#     token=os.environ['DATABRICKS_TOKEN'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# get sql warehouse connection\n",
    "###################################################\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "log.info('creating sql warehouse connection')\n",
    "connection = sql.connect(\n",
    "    server_hostname = os.getenv(\"DATABRICKS_HOST\"),\n",
    "    http_path       = os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "    access_token    = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b8a7e4-a32c-483e-b417-28bce69c846f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# get data from Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_df(query, connection):\n",
    "    \n",
    "    log.info('executing query')\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    log.info('fetching results')\n",
    "    result = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    df = pd.DataFrame(result, columns=columns)\n",
    "    cursor.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "facd06ea-42fc-4b00-a130-6953134b85ff",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# refresh_data = True\n",
    "refresh_data = (\n",
    "    'lmp_df.parquet' not in os.listdir() or\n",
    "    'mtlf_df.parquet' not in os.listdir() or\n",
    "    'mtrf_df.parquet' not in os.listdir()\n",
    ")\n",
    "refresh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8edd14c-9054-443d-af7d-ca757e85e10d",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## lmp\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM prd_landing_zone.spp_weis.lmp_hourly'\n",
    "    # res = spark.sql(query).collect()\n",
    "    # df = spark.createDataFrame(res).toPandas()\n",
    "    # to parquet stopped working with serializeable json error\n",
    "\n",
    "    df = get_query_df(query, connection)\n",
    "    \n",
    "    df.to_feather('lmp_df.parquet')\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_feather('test_lmp_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0e6d66-a7b0-4525-a93c-952cee98ae33",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mtrf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM prd_landing_zone.spp_weis.mtrf'\n",
    "    # res = spark.sql(query).collect()\n",
    "    # df = spark.createDataFrame(res).toPandas()\n",
    "\n",
    "    df = get_query_df(query, connection)\n",
    "    \n",
    "    df.to_feather('mtrf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a953c5a1-6b78-499f-aa5e-e0742d4eaa8c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mtlf\n",
    "if refresh_data:\n",
    "    query = 'SELECT * FROM prd_landing_zone.spp_weis.mtlf'\n",
    "    # res = spark.sql(query).collect()\n",
    "    # df = spark.createDataFrame(res).toPandas()\n",
    "    \n",
    "    df = get_query_df(query, connection)\n",
    "    \n",
    "    df.to_feather('mtlf_df.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe84437-f726-4282-a522-a8e69d598674",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80032709-ef03-4d4e-8463-9cf8acd27b77",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmp_df = pd.read_feather('lmp_df.parquet')\n",
    "mtlf_df = pd.read_feather('mtlf_df.parquet').sort_values('GMTIntervalEnd')\n",
    "mtrf_df = pd.read_feather('mtrf_df.parquet').sort_values('GMTIntervalEnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeries.from_dataframe returns the following error if there is a timezone\n",
    "# TypeError: Cannot interpret 'datetime64[ns, UTC]' as a data type\n",
    "lmp_df.GMTIntervalEnd = lmp_df.GMTIntervalEnd.dt.tz_localize(None)\n",
    "lmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be88d18-c76f-4cb2-9aae-da72ef8e79e7",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mtlf_df.Interval = mtlf_df.Interval.dt.tz_localize(None)\n",
    "mtlf_df.GMTIntervalEnd = mtlf_df.GMTIntervalEnd.dt.tz_localize(None)\n",
    "mtlf_df.timestamp  = mtlf_df.timestamp.dt.tz_localize(None)\n",
    "mtlf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrf_df.Interval = mtrf_df.Interval.dt.tz_localize(None)\n",
    "mtrf_df.GMTIntervalEnd = mtrf_df.GMTIntervalEnd.dt.tz_localize(None)\n",
    "mtrf_df.timestamp  = mtrf_df.timestamp.dt.tz_localize(None)\n",
    "mtrf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74464c68-0b26-4511-877d-3aa4d50d0f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47977262-0c35-41fd-b9c7-a68a3a30afa2",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lmp\n",
    "psco_lmp_df, list_nodes_name, psco_price_df_long = de.get_psco_price_df(lmp_df)\n",
    "lmp_series = de.create_psco_price_series(psco_lmp_df, list_nodes_name)\n",
    "\n",
    "# remove duplicates\n",
    "dups = mtlf_df.GMTIntervalEnd.duplicated()\n",
    "log.info(f'mtlf_df duplicated: {dups.sum()}')\n",
    "mtlf_df = mtlf_df[~dups]\n",
    "\n",
    "dups = mtrf_df.GMTIntervalEnd.duplicated()\n",
    "log.info(f'mtrf_df duplicated: {dups.sum()}')\n",
    "mtrf_df = mtrf_df[~dups]\n",
    "    \n",
    "# mtlf series\n",
    "mtlf_series, avg_act_series = de.create_mtlf_series(mtlf_df)\n",
    "# mtlf_series, avg_act_series = de.create_mtlf_lmp_series(mtlf_df, psco_lmp_df, list_nodes_name)\n",
    "\n",
    "# mtrf series\n",
    "mtrf_ratio_df = de.add_enrgy_ratio_to_mtrf(mtlf_df, mtrf_df)\n",
    "mtrf_ratio_df = de.add_enrgy_ratio_diff_to_mtrf(mtrf_ratio_df)\n",
    "mtrf_series = de.create_mtrf_series(mtrf_ratio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrf_ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df53842-f665-4fa9-bf7d-6bf62b80b007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_nodes_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9970e50-790b-4966-87ba-81d297bf3125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preprocess series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3671d03a-bb10-4e3e-bdc3-5045182843ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "start_time = pd.Timestamp('2023-04-02 00:00:00')\n",
    "### TIME CHANGE ########################################################\n",
    "N_FCAST_DAYS = 6\n",
    "input_chunk_length = 2*N_FCAST_DAYS*24\n",
    "forecast_horizon = 24*N_FCAST_DAYS\n",
    "# training_cutoff = pd.Timestamp(\"2023-06-01 06:00:00\")\n",
    "training_cutoff = de.get_train_cutoff(lmp_series, forecast_horizon)\n",
    "print(f'training_cutoff: {training_cutoff}')\n",
    "########################################################################\n",
    "lmp_series = de.lmp_series_drop_horizon(lmp_series, start_time, forecast_horizon)\n",
    "# training_cutoff = de.get_train_cutoff(lmp_series_drop_horizon)\n",
    "lmp_series_train, lmp_series_val, lmp_series_all = de.get_lmp_train_test_series(lmp_series, training_cutoff, forecast_horizon, input_chunk_length)\n",
    "(lmp_series_train_transformed, \n",
    " lmp_series_val_transformed, \n",
    " lmp_series_transformed,\n",
    " lmp_scaler) = de.scale_series(lmp_series_train, lmp_series_val, lmp_series_all, global_fit=True)\n",
    "scalers['series'] = lmp_scaler\n",
    "\n",
    "print(f'train start: {lmp_series_train.start_time()}')\n",
    "print(f'train end: {lmp_series_train.end_time()}')\n",
    "print(f'val start: {lmp_series_val.start_time()}')\n",
    "print(f'val end: {lmp_series_val.end_time()}')\n",
    "\n",
    "\n",
    "mtlf_series_train, mtlf_series_val, mtlf_series = de.get_mtlf_train_test_series(\n",
    "    mtlf_series, start_time, training_cutoff, forecast_horizon, input_chunk_length\n",
    "    )\n",
    "# (mtlf_series_train_transformed, \n",
    "#  mtlf_series_val_transformed, \n",
    "#  mtlf_series_transformed, \n",
    "#  mtlf_scaler) = de.scale_series(mtlf_series_train, mtlf_series_val, mtlf_series)\n",
    "# scalers['mtlf'] = mtlf_scaler\n",
    "\n",
    "# print(f'train start: {mtlf_series_train.start_time()}')\n",
    "# print(f'train end: {mtlf_series_train.end_time()}')\n",
    "# print(f'val start: {mtlf_series_val.start_time()}')\n",
    "# print(f'val end: {mtlf_series_val.end_time()}')\n",
    "\n",
    "\n",
    "avg_act_series_train, avg_act_series_val, avg_act_series = de.get_avg_act_train_test_series(avg_act_series, start_time, training_cutoff)\n",
    "(avg_act_series_train_transformed, \n",
    " avg_act_series_val_transformed, \n",
    " avg_act_series_transformed,\n",
    " past_scaler) = de.scale_series(avg_act_series_train, avg_act_series_val, avg_act_series)\n",
    "scalers['pc'] = past_scaler\n",
    "\n",
    "print(f'train start: {avg_act_series_train.start_time()}')\n",
    "print(f'train end: {avg_act_series_train.end_time()}')\n",
    "print(f'val start: {avg_act_series_val.start_time()}')\n",
    "print(f'val end: {avg_act_series_val.end_time()}')\n",
    "\n",
    "\n",
    "mtrf_series_train, mtrf_series_val, mtrf_series = de.get_mtrf_train_test_series(\n",
    "    mtrf_series, start_time, training_cutoff, forecast_horizon, input_chunk_length\n",
    "    )\n",
    "# mtrf_series_train_transformed, mtrf_series_val_transformed, mtrf_series_transformed = de.scale_mtrf_series(mtrf_series_train, mtrf_series_val, mtrf_series)\n",
    "\n",
    "# print(f'train start: {mtrf_series_train.start_time()}')\n",
    "# print(f'train end: {mtrf_series_train.end_time()}')\n",
    "# print(f'val start: {mtrf_series_val.start_time()}')\n",
    "# print(f'val end: {mtrf_series_val.end_time()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7197b-a709-4c71-9576-67445bfce11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "past_cov_train = avg_act_series_train_transformed\n",
    "past_cov_val = avg_act_series_val_transformed\n",
    "past_cov = avg_act_series_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7068e6c7-8c21-4d41-bbd0-974c15bffa9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future training covariates\n",
    "future_covariates_train = concatenate([mtlf_series_train, mtrf_series_train], axis=1)\n",
    "future_covariates_train.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e761aa-008f-4772-b8b5-18093014eec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate future validation covariates\n",
    "end_time = mtlf_series_val.end_time() + pd.Timedelta('1H')\n",
    "mtrf_series_val_end_droped = mtrf_series_val.drop_after(end_time)\n",
    "\n",
    "future_covariates_val = concatenate([mtlf_series_val, mtrf_series_val_end_droped], axis=1)\n",
    "future_covariates_val.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2187efbc-bfaa-4d0e-a9ee-3dcde69e5076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the entire covariate series\n",
    "mtrf_series_end_droped = mtrf_series.drop_after(end_time)\n",
    "\n",
    "future_covariates = concatenate([mtlf_series, mtrf_series_end_droped], axis=1)\n",
    "future_covariates.values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e52eb663-bc36-45d8-97d9-0b3cbb722e2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(future_covariates_train_transformed, \n",
    " future_covariates_val_transformed, \n",
    " future_covariates_transformed, \n",
    " future_scaler) = de.scale_series(future_covariates_train, future_covariates_val, future_covariates)\n",
    "scalers['fc'] = future_scaler\n",
    "\n",
    "print(f'train start: {future_covariates_train.start_time()}')\n",
    "print(f'train end: {future_covariates_train.end_time()}')\n",
    "print(f'val start: {future_covariates_val.start_time()}')\n",
    "print(f'val end: {future_covariates_val.end_time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6e78b9-1b3f-44fd-b773-09854dcf3d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lmp_train_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_train_all.append(lmp_series_train_transformed[list_nodes_name[i]])\n",
    "\n",
    "lmp_val_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_val_all.append(lmp_series_val_transformed[list_nodes_name[i]])\n",
    "\n",
    "lmp_all = []\n",
    "for i in range(len(list_nodes_name)):\n",
    "    lmp_all.append(lmp_series_transformed[list_nodes_name[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa23eb3-e67f-47ef-b6be-cea9e98a019d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4fab0c-ea6a-4681-8639-bd7366f86ea3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6283044-f609-4a22-901d-980f776b41da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Global model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ef3cdf-10c2-4954-a7ff-476b5b99036d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Considerations**\n",
    "- Target series: `LMP`\n",
    "- Past Covariate series: `Averaged_actula` \n",
    "- Future Covariate Series: `MTLF`, `Wind`, and `Solar`. Basically, these are mid-term load forecasts and mid-term renewables forecasts.\n",
    "- `input_chunk-length = 168 (7 days)`, `output_chunk_length = 24 hours` and **forecasting horizon could be the length of output_chunk or my validation set?**\n",
    "\n",
    "##### Note : You can pass the covariates to the model as individual series in a list, or you can stack them and pass them as one multivariate series. However, you cannot sack them if they do not have the same length.\n",
    "##### Note from Dart's TFT example: we can just provide the ***whole covariates series*** as future_covariates argument to the model; the model will slice these covariates and use only what it needs in order to train on forecasting the target train_transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22cd3f2a-25f6-4af8-85e7-b986c602a50a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit the model using covariate series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6539e838-ae87-4b39-b036-d4d3e4b95b78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_samples = 200\n",
    "figsize = (16, 5)\n",
    "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
    "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
    "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c085b74-b3d2-49b8-8489-5bfb9231fd02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default quantiles for QuantileRegression\n",
    "quantiles = [\n",
    "    0.01,\n",
    "    0.05,\n",
    "    0.1,\n",
    "    0.15,\n",
    "    0.2,\n",
    "    0.25,\n",
    "    0.3,\n",
    "    0.4,\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    0.99,\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2ce9a9-0900-4e9b-9289-85c32a1cf92d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccfe2da-4efb-414a-b4b1-a9fe04ea0917",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Log the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a271783b-c518-4cf7-8b06-124d6641d7d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set up MLFlow experiment\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking.html#logging-to-a-tracking-server\n",
    "\n",
    "need to set env vars `MLFLOW_TRACKING_URI` and `MLFLOW_TRACKING_TOKEN` to access tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b861062-799c-4c85-9771-93b3f8ab9799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb16de9-f161-4730-8f19-0dc0b7cb88dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m exp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspp_weis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m exp_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./mlflow_exp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_experiment(exp_name, exp_location)\n\u001b[1;32m      4\u001b[0m exp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "exp_name = 'Justin.L.Fields@xcelenergy.com'\n",
    "experiment_name = '.'\n",
    "experiment_path = f\"/Users/{user_name}/{experiment_name}\"\n",
    "exp = mlflow.create_experimentexperiment_path)\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b571fe6-96ee-49c6-bcfa-ae75329840c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exp.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7348dd-7bff-4c2a-9690-f7ae84243823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exp.artifact_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0142a3d-9039-45ae-85ab-8e36dc350eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b009baa3-aa46-46ff-be1e-c89cf591a5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Infer signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f046952a-03d9-48d4-80d8-0f4864033daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create an input example to infer signature\n",
    "node_series = lmp_series[list_nodes_name[1]]\n",
    "past_cov_series = avg_act_series\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "data = {\n",
    "    'series': [node_series.to_json()],\n",
    "    'past_covariates': [past_cov_series.to_json()],\n",
    "    'future_covariates': [future_cov_series.to_json()],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': 200\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "ouput_example = 'the endpoint return json as a string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b588a7dc-bf97-4bed-b85c-a02ef12ad35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this signature will be logged with the model\n",
    "# registered models must have signatures\n",
    "from mlflow.models import infer_signature\n",
    "darts_tft_signature = infer_signature(df, ouput_example)\n",
    "darts_tft_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90938516-5aa4-42a9-aaa3-3bdea7978ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Select training series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391287b3-dc78-448f-8419-11b323552118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select series for training\n",
    "num_training_series = min(20, len(lmp_train_all))\n",
    "training_series = [lmp_train_all[i] for i in range(num_training_series)]\n",
    "val_series = [lmp_val_all[i] for i in range(num_training_series)]\n",
    "# for final training on all data\n",
    "all_series = [lmp_all[i] for i in range(num_training_series)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a0760c-c5e4-4d0e-aa68-c80dda4ca982",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train and log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.log_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ccb07b-e244-4ad0-b1b5-fa9d21020126",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can turn off backtesting to save time for testing code\n",
    "run_backtest = True\n",
    "\n",
    "# torch_metrics = MeanAbsolutePercentageError()\n",
    "torch_metrics = MeanAbsoluteError()\n",
    "# torch_metrics = MeanSquaredError(squared=False)\n",
    "\n",
    "artifact_path = \"model_artifacts\"\n",
    "\n",
    "encoders = {\n",
    "    # \"cyclic\": {\n",
    "    #     \"past\": [\"month\"], \n",
    "    #     \"future\": [\"month\"]\n",
    "    # },\n",
    "    \"datetime_attribute\": {\n",
    "        \"future\": [\"hour\", \"dayofweek\", \"month\"], # \n",
    "        \"past\": [\"hour\", \"dayofweek\", \"month\"], # \n",
    "    },\n",
    "    \"position\": {\n",
    "        \"past\": [\"relative\"], \n",
    "        \"future\": [\"relative\"]\n",
    "    },\n",
    "    \"transformer\": Scaler()\n",
    "}\n",
    "\n",
    "# MODEL_TYPE = \"tft_model\"\n",
    "# MODEL_TYPE = \"tide_model\"\n",
    "# MODEL_TYPE = \"dlinear_model\"\n",
    "# MODEL_TYPE = \"nlinear_model\"\n",
    "MODEL_TYPE = \"ts_mixer_model\"\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    print(f'run.info: \\n{run.info}')\n",
    "\n",
    "    # common model parameters\n",
    "    lr = 1e-4\n",
    "    batch_size = 64\n",
    "    n_epochs = 4\n",
    "    n_epochs_final = 2\n",
    "    refit = True\n",
    "    final_train_val_only = True\n",
    "\n",
    "    # set up covariate series\n",
    "    past_covariates = [past_cov for i in range(num_training_series)]\n",
    "    future_covariates=[future_covariates_transformed for i in range(num_training_series)]\n",
    "\n",
    "    # common parameters across models\n",
    "    model_params = {\n",
    "        'input_chunk_length': input_chunk_length,\n",
    "        'output_chunk_length': forecast_horizon,\n",
    "        'batch_size': batch_size,\n",
    "        'n_epochs': n_epochs,\n",
    "        'add_encoders': encoders,\n",
    "        'likelihood': QuantileRegression(quantiles=quantiles),  # QuantileRegression is set per default\n",
    "        'optimizer_kwargs': {\"lr\": lr},\n",
    "        'random_state': 42,\n",
    "        'torch_metrics': torch_metrics,\n",
    "    }\n",
    "\n",
    "    if MODEL_TYPE == \"tft_model\":\n",
    "        # https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tft_model.html#temporal-fusion-transformer-tft\n",
    "        #TFT\n",
    "        hidden_size = 16 # Hidden state size of the TFT. It is the main hyper-parameter and common across the internal TFT architecture.\n",
    "        lstm_layers = 1 # Number of layers for the Long Short Term Memory (LSTM) Encoder and Decoder (1 is a good default).\n",
    "        num_attention_heads = 2 # Number of attention heads (4 is a good default)\n",
    "    \n",
    "        model_params.update({\n",
    "            'hidden_size': hidden_size,\n",
    "            'lstm_layers': lstm_layers,\n",
    "            'num_attention_heads': num_attention_heads,\n",
    "            'dropout': 0.1,\n",
    "            })\n",
    "    \n",
    "        model = TFTModel(**model_params)\n",
    "\n",
    "    elif MODEL_TYPE == \"tide_model\":\n",
    "        # https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tide_model.html#time-series-dense-encoder-tide\n",
    "\n",
    "        #TiDE\n",
    "        num_encoder_layers=1 # The number of residual blocks in the encoder.\n",
    "        num_decoder_layers=1 # The number of residual blocks in the decoder.\n",
    "        decoder_output_dim=16 # The dimensionality of the output of the decoder.\n",
    "        hidden_size=64 # The width of the layers in the residual blocks of the encoder and decoder.\n",
    "        temporal_width_past=0 # The width of the layers in the past covariate projection residual block. If 0, will bypass feature projection and use the raw feature data\n",
    "        temporal_width_future=0 # The width of the layers in the future covariate projection residual block. If 0, will bypass feature projection and use the raw feature data.\n",
    "        temporal_decoder_hidden=32 # The width of the layers in the temporal decoder.\n",
    "    \n",
    "        model_params.update({\n",
    "            'num_encoder_layers': num_encoder_layers,\n",
    "            'num_decoder_layers': num_decoder_layers,\n",
    "            'decoder_output_dim': decoder_output_dim,\n",
    "            'hidden_size': hidden_size,\n",
    "            'temporal_width_past': temporal_width_past,\n",
    "            'temporal_width_future': temporal_width_future,\n",
    "            'temporal_decoder_hidden': temporal_decoder_hidden,\n",
    "            'dropout': 0.1,\n",
    "            })\n",
    "        \n",
    "        model = TiDEModel(**model_params)\n",
    "\n",
    "    elif MODEL_TYPE == \"dlinear_model\":\n",
    "        # https://unit8co.github.io/darts/generated_api/darts.models.forecasting.dlinear.html#d-linear\n",
    "        # dlinear\n",
    "        kernel_size=5\n",
    "    \n",
    "        model_params.update({\n",
    "            'kernel_size': kernel_size,\n",
    "            })\n",
    "        \n",
    "        model = DLinearModel(**model_params)\n",
    "\n",
    "    elif MODEL_TYPE == \"ts_mixer_model\":\n",
    "        # https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tsmixer_model.html\n",
    "        model_params.update({\n",
    "            'hidden_size': 32,\n",
    "            'ff_size': 64,\n",
    "            'num_blocks': 2\n",
    "            })\n",
    "        \n",
    "        model = TSMixerModel(**model_params)\n",
    "\n",
    "    else:\n",
    "        # https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nlinear.html#n-linear\n",
    "        # nlinear doesn't require additional parameters\n",
    "        model = NLinearModel(**model_params)\n",
    "        \n",
    "    fit_params = {\n",
    "        'series': training_series,\n",
    "        'val_series': val_series,\n",
    "        'past_covariates': past_covariates,\n",
    "        'val_past_covariates': past_covariates,\n",
    "        'future_covariates': future_covariates,\n",
    "        'val_future_covariates': future_covariates,\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(**fit_params)\n",
    "\n",
    "    # log parameters for the run\n",
    "    # need to add accuracy results here...\n",
    "    params = {\n",
    "        \"model_type\": MODEL_TYPE,\n",
    "        \"lr\":lr,\n",
    "        \"epochs_trained\": model.epochs_trained,\n",
    "        \"n_epochs_final\": n_epochs_final,\n",
    "        \"refit\":refit,\n",
    "        \"final_train_val_only\": final_train_val_only,\n",
    "        \"num_training_series\": num_training_series,\n",
    "        }\n",
    "\n",
    "    # add model params\n",
    "    params = params | model_params\n",
    "    metrics = {}\n",
    "\n",
    "    # backtesting takes a moment and generates a lot of output\n",
    "    # we can turn it off for testing\n",
    "    if run_backtest:\n",
    "        # back test on validation data\n",
    "        acc = model.backtest(\n",
    "            series=val_series,\n",
    "            # series=all_series,\n",
    "            past_covariates=past_covariates,\n",
    "            future_covariates=future_covariates,\n",
    "            retrain=False,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            stride=25,\n",
    "            metric=[mae, rmse],\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        acc_df = pd.DataFrame(\n",
    "            np.mean(acc, axis=0).reshape(1,-1),\n",
    "            columns=['mae', 'rmse']\n",
    "        )\n",
    "\n",
    "        # log metrics\n",
    "        metrics['mae'] = acc_df.mae[0]\n",
    "        metrics['rmse'] = acc_df.rmse[0]\n",
    "\n",
    "\n",
    "    # finish training on entire data set before logging model\n",
    "    if final_train_val_only:\n",
    "        final_train_series = val_series\n",
    "    else:\n",
    "        # for final training on all data\n",
    "        final_train_series = all_series\n",
    "\n",
    "    if refit:\n",
    "        log.info('final training')\n",
    "        model.fit(\n",
    "                series=final_train_series,\n",
    "                past_covariates=past_covariates,\n",
    "                future_covariates=future_covariates,\n",
    "                verbose=True,\n",
    "                epochs=n_epochs_final, # continue training\n",
    "                )\n",
    "    \n",
    "    # final model back test on validation data\n",
    "    acc = model.backtest(\n",
    "            series=val_series,\n",
    "            past_covariates=past_covariates,\n",
    "            future_covariates=future_covariates,\n",
    "            retrain=False,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            stride=25,\n",
    "            metric=[mae, rmse],\n",
    "            verbose=False,\n",
    "        )\n",
    "    \n",
    "    acc_df = pd.DataFrame(\n",
    "        np.mean(acc, axis=0).reshape(1,-1),\n",
    "        columns=['mae', 'rmse']\n",
    "    )\n",
    "\n",
    "    # log metrics\n",
    "    metrics['mae_final'] = acc_df.mae[0]\n",
    "    metrics['rmse_final'] = acc_df.rmse[0]\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # set up path to save model\n",
    "    model_path = '/'.join([artifact_path, MODEL_TYPE])\n",
    "\n",
    "    shutil.rmtree(artifact_path, ignore_errors=True)\n",
    "    os.makedirs(artifact_path)\n",
    "\n",
    "    # log params\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # save model files (model, model.ckpt) \n",
    "    # and load them to artifacts when logging the model\n",
    "    model.save(model_path)\n",
    "\n",
    "    # save scalers to artifacts\n",
    "    scaler_name = 'scalers.pkl'\n",
    "    scaler_path = '/'.join([artifact_path, scaler_name])\n",
    "    with open(scaler_path, 'wb') as handle:\n",
    "        pickle.dump(scalers, handle)\n",
    "\n",
    "    # save MODEL_TYPE to artifacts\n",
    "    # this will be used to load the model from the artifacts\n",
    "    model_type_path = '/'.join([artifact_path, 'MODEL_TYPE.pkl'])\n",
    "    with open(model_type_path, 'wb') as handle:\n",
    "        pickle.dump(MODEL_TYPE, handle)\n",
    "    \n",
    "\n",
    "    # map model artififacts in dictionary\n",
    "    artifacts = {\n",
    "        'model': model_path,\n",
    "        'model.ckpt': model_path+'.ckpt',\n",
    "        'scalers': scaler_path,\n",
    "        'MODEL_TYPE': model_type_path,\n",
    "        'lmp_df': 'lmp_df.parquet',\n",
    "        'mtlf_df': 'mtlf_df.parquet',\n",
    "        'mtrf_df': 'mtrf_df.parquet'\n",
    "    }\n",
    "\n",
    "    \n",
    "    # log model\n",
    "    # https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html#pip-requirements-example\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path='GlobalForecasting',\n",
    "        code_path=['notebooks/model_training/darts_wrapper.py'],\n",
    "        signature=darts_tft_signature,\n",
    "        artifacts=artifacts,\n",
    "        # model will get loaded from artifacts, we don't need instantiate with one\n",
    "        python_model=DartsGlobalModel(), \n",
    "        pip_requirements=[\"-r notebooks/model_training/requirements.txt\"],\n",
    "    )\n",
    "\n",
    "    ## logging the parquet file as an artifact seems to work better\n",
    "    # mlflow.log_table(data=lmp_df, artifact_file=\"lmp_df.parquet\")\n",
    "    # mlflow.log_table(data=mtlf_df, artifact_file=\"mtlf_df.parquet\")\n",
    "    # mlflow.log_table(data=mtrf_df, artifact_file=\"mtrf_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cb6813-c05d-4eff-b80b-00594654ec21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe29a84-c6be-42b7-a185-84aa37f0d46a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?TFTExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    i=3\n",
    "    test_node_name = list_nodes_name[i]\n",
    "    print(test_node_name)\n",
    "    \n",
    "    test_series = all_series[i]\n",
    "    \n",
    "    \n",
    "    future_covariates = concatenate([mtlf_series, mtrf_series_end_droped], axis=1)\n",
    "    future_covariates.values().shape\n",
    "    \n",
    "    past_cov_series = avg_act_series\n",
    "    future_cov_series = future_covariates\n",
    "\n",
    "    background_series = test_series.drop_after(test_series.end_time() - pd.Timedelta('200H'))\n",
    "    \n",
    "    from darts.explainability import TFTExplainer\n",
    "    explainer = TFTExplainer(\n",
    "        model=model, \n",
    "        background_series=background_series, \n",
    "        background_past_covariates=past_cov_series,\n",
    "        background_future_covariates=future_cov_series)\n",
    "\n",
    "    explainability_result = explainer.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainer.plot_variable_selection(explainability_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainer.plot_attention(explainability_result, plot_type=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer.plot_attention(explainability_result, plot_type=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainer.plot_attention(explainability_result, plot_type=\"heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainability_result.get_encoder_importance().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainability_result.get_decoder_importance().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    explainability_result.get_static_covariates_importance().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"tft_model\":\n",
    "    attention = explainability_result.get_attention().mean(axis=1)\n",
    "\n",
    "    time_intersection = test_series.time_index.intersection(attention.time_index)\n",
    "    \n",
    "    test_series[time_intersection].plot()\n",
    "    attention.plot(label=\"mean_attention\", max_nr_components=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8a9662-8e19-4342-9ed3-e73f6a5ef934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load the model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a767725d-e5e2-4039-9fd3-16e47ddbb33c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "# use client to get run information from the experiment\n",
    "client = MlflowClient()\n",
    "\n",
    "exp = client.get_experiment_by_name(experiment_path)\n",
    "exp.experiment_id\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids = exp.experiment_id,\n",
    "    order_by=['param.mae']\n",
    "    )\n",
    "\n",
    "# runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3266744f-a20f-4f4b-be5a-3d2602623270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run.to_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e657ef23-d71c-4331-ab8d-447b7fcfa53d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine the run information into a model_log_df\n",
    "info_df = pd.DataFrame([r.to_dictionary()['info'] for r in runs])\n",
    "metrics_df = pd.DataFrame([r.to_dictionary()['data']['metrics'] for r in runs])\n",
    "params_df = pd.DataFrame([r.to_dictionary()['data']['params'] for r in runs])\n",
    "model_log_df = pd.concat([info_df, metrics_df, params_df], axis=1)\n",
    "model_log_df.sort_values('mae', ascending=True, inplace=True)\n",
    "model_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0891ebe-d05e-47ff-8a19-ea0f622bb557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sort on on end_time to get the latest run\n",
    "model_log_df.sort_values('end_time', ascending=False, inplace=True)\n",
    "model_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc83b57-3fae-4e14-b954-9fe0b92b2e45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: need to update so only logs model if it's not already logged\n",
    "best_run_id = model_log_df.run_id.iloc[0]\n",
    "best_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logged_model = 'runs:/7642b08bcbe14af2a34b0fdb2eeafade/GlobalForecasting' # v45\n",
    "# logged_model = 'runs:/6d3c5b0d1b7742a7b2b682b8efe97463/GlobalForecasting' # v47\n",
    "# logged_model = 'runs:/934ae15de0904b8e9836b2e75cb7ffbf/GlobalForecasting' # v47\n",
    "logged_model = f'runs:/{best_run_id}/GlobalForecasting'\n",
    "\n",
    "# Load model as a PyFuncModel\n",
    "# we can use this to test our custom class\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test custom end point - get predicitions and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_series[i]#.concatenate(val_series[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "test_node_name = list_nodes_name[i]\n",
    "print(test_node_name)\n",
    "\n",
    "test_series = all_series[i]\n",
    "# test_series = lmp_all[i]\n",
    "# test_series = val_series[i]\n",
    "# test_end_time = min(test_series.end_time(), pd.Timestamp('2023-10-15T12:00:00'))\n",
    "# test_end_time = min(test_series.end_time(), pd.Timestamp('2023-09-29T23:00:00'))\n",
    "# test_end_time = min(test_series.end_time(), pd.Timestamp('2024-04-01T23:00:00'))\n",
    "# test_end_time = min(test_series.end_time(), pd.Timestamp('2024-04-15T23:00:00'))\n",
    "test_end_time = min(test_series.end_time(), pd.Timestamp('2024-07-01T23:00:00'))\n",
    "print(test_end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrf_series_end_droped = mtrf_series.drop_after(end_time)\n",
    "future_covariates = concatenate([mtlf_series, mtrf_series_end_droped], axis=1)\n",
    "future_covariates.values().shape\n",
    "\n",
    "# if test_end_time < test_series.end_time():\n",
    "node_series = test_series.drop_after(test_end_time)\n",
    "    \n",
    "log.info(f'test_end_time: {test_end_time}')\n",
    "log.info(f'node_series.end_time(): {node_series.end_time()}')\n",
    "past_cov_series = avg_act_series\n",
    "future_cov_series = future_covariates\n",
    "\n",
    "\n",
    "data = {\n",
    "    'series': [node_series.to_json()],\n",
    "    'past_covariates': [past_cov_series.to_json()],\n",
    "    'future_covariates': [future_cov_series.to_json()],\n",
    "    'n': forecast_horizon,\n",
    "    'num_samples': 200\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plot_cov_df = future_cov_series.pd_dataframe()\n",
    "plot_cov_df = plot_cov_df.reset_index().rename(columns={'GMTIntervalEnd':'time'})\n",
    "plot_cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84af96a4-5ab5-4068-ad12-7300a5078e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict on a Pandas DataFrame.\n",
    "df['num_samples'] = 500\n",
    "pred = loaded_model.predict(df)\n",
    "preds = TimeSeries.from_json(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84af96a4-5ab5-4068-ad12-7300a5078e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_df = (preds.pd_dataframe()\n",
    "        .reset_index()\n",
    "        .melt(id_vars='time')\n",
    "        .rename(columns={'component':'node'})\n",
    ")\n",
    "\n",
    "temp_df['sim_num'] = [int(n.split('_')[-1].replace('s', '')) for n in temp_df.node]\n",
    "temp_df.node = ['_'.join(n.split('_')[:-1]) for n in temp_df.node]\n",
    "\n",
    "temp_df_pivot = temp_df.reset_index().pivot(index=['time', 'node'], columns='sim_num', values='value')\n",
    "temp_df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84af96a4-5ab5-4068-ad12-7300a5078e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q_df = plotting.get_quantile_df(preds)\n",
    "q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84af96a4-5ab5-4068-ad12-7300a5078e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_df = plotting.get_mean_df(preds).merge(\n",
    "    plotting.get_quantile_df(preds),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "plot_df = plotting.get_plot_df(\n",
    "        TimeSeries.from_json(pred),\n",
    "        plot_cov_df,\n",
    "        lmp_df.rename(columns={'PNODE_Name':'node', 'GMTIntervalEnd':'time'}),\n",
    "        test_node_name,\n",
    "    )\n",
    "plot_df.rename(columns={'mean':'mean_fcast'}, inplace=True)\n",
    "plot_df\n",
    "\n",
    "plotting.plotly_forecast(plot_df, test_node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f20f13f-f2b1-4a69-b33a-389650aca00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `plotting.get_plot_df` not found.\n"
     ]
    }
   ],
   "source": [
    "plotting.get_plot_df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e841b4-aa93-45d4-bbc1-ef2756dbeb0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b35ed2-227c-4f22-b4c6-116f66d818a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592b4145-2290-43c6-be0d-21f10f72e8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "#TODO: register model only if it hasn't been logged yet\n",
    "    catalog = \"sandbox_data_science\"\n",
    "    schema = \"spp_weis\"\n",
    "    model_name = \"GlobalForecasting\"\n",
    "    \n",
    "    registry_uri = \"databricks-uc\"\n",
    "    mlflow.set_registry_uri(registry_uri)\n",
    "    mlflow.register_model(\n",
    "        model_uri=logged_model,\n",
    "        name=f\"{catalog}.{schema}.{model_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1b1269-5cca-42e6-8f05-c401b1bb1538",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transistion models stage\n",
    "# https://mlflow.org/docs/latest/model-registry.html#transitioning-an-mlflow-models-stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f5a6ad-eb16-4bfb-a495-72525185db4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for programtic deployments see\n",
    "# https://docs.databricks.com/_extras/notebooks/source/machine-learning/model-serving-endpoint-python.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64588d01-a870-4a52-98f5-2787d42ab9e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc14ad4-f4c3-4fcd-b060-a0891a16a7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prediction using API endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae44bfa-7065-491d-8f59-8bd3a5413b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# def create_tf_serving_json(data):\n",
    "#   return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "# def score_model(dataset):\n",
    "#   url = 'https://dbc-beada314-1494.cloud.databricks.com/serving-endpoints/spp_weis/invocations'\n",
    "#   api_token = 'dapi0744c2d5e8ed2b39576805ba0ad5f692'\n",
    "#   headers = {'Authorization': f'Bearer {api_token}', 'Content-Type': 'application/json'}\n",
    "#   ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "#   data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "#   response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "#   if response.status_code != 200:\n",
    "#     raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "#   return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d46028-35b7-4188-b7f8-82c08ab685aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# endpoint_pred = score_model(df)\n",
    "# endpoint_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72892c3c-f8f4-4877-b6c1-aca4b3b95bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# preds = TimeSeries.from_json(endpoint_pred['predictions'])\n",
    "# preds.mean(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d9fdc9-65ab-4082-818d-b230c231f3e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb737393-3f08-4e25-9776-f253395bfb0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WorkspaceClient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 705378560806507,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow_log_register_jlf",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
