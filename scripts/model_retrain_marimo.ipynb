{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "from typing import List\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from darts.models import (\n",
    "    TFTModel,\n",
    "    TiDEModel,\n",
    "    TSMixerModel,\n",
    "    NaiveEnsembleModel,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"model_retrain\")\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project root so src/ imports work regardless of where\n",
    "# the notebook is launched from (local dev, Databricks, CI, etc.)\n",
    "while 'src' not in os.listdir():\n",
    "    os.chdir('..')\n",
    "\n",
    "src_path = \"src\"\n",
    "if os.path.isdir(src_path) and src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "log.info(f\"os.getcwd(): {os.getcwd()}\")\n",
    "\n",
    "import data_engineering as de\n",
    "import parameters\n",
    "import utils\n",
    "from modeling import build_fit_tsmixerx, build_fit_tft, build_fit_tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Databricks, pull AWS creds from the secrets store.\n",
    "# Locally, fall back to .env file via python-dotenv.\n",
    "if 'dbutils' in locals():\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope = \"aws\", key = \"AWS_ACCESS_KEY_ID\")\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope = \"aws\", key = \"AWS_SECRET_ACCESS_KEY\")\n",
    "    os.environ['AWS_S3_BUCKET'] = dbutils.secrets.get(scope = \"aws\", key = \"AWS_S3_BUCKET\")\n",
    "    os.environ['AWS_S3_FOLDER'] = dbutils.secrets.get(scope = \"aws\", key = \"AWS_S3_FOLDER\")\n",
    "    import mlflow\n",
    "    mlflow.autolog(disable=True)\n",
    "else:\n",
    "    print('not on DBX')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(f\"FORECAST_HORIZON: {parameters.FORECAST_HORIZON}\")\n",
    "log.info(f\"INPUT_CHUNK_LENGTH: {parameters.INPUT_CHUNK_LENGTH}\")\n",
    "log.info(f\"MODEL_NAME: {parameters.MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\")\n",
    "AWS_S3_FOLDER = os.getenv(\"AWS_S3_FOLDER\")\n",
    "log.info(f'{AWS_S3_FOLDER = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {},
   "source": [
    "## Connect to database and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DuckDB connection backed by S3 parquet files\n",
    "con = de.create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"preparing lmp data\")\n",
    "lmp = de.prep_lmp(con)\n",
    "lmp_df = lmp.to_pandas().rename(\n",
    "    columns={\n",
    "        \"LMP\": \"LMP_HOURLY\",\n",
    "        \"unique_id\": \"node\",\n",
    "        \"timestamp_mst\": \"time\",\n",
    "    }\n",
    ")\n",
    "\n",
    "log.info(\"preparing covariate data\")\n",
    "all_df_pd = de.all_df_to_pandas(de.prep_all_df(con))\n",
    "all_df_pd.info()\n",
    "\n",
    "lmp_all, train_all, test_all, train_test_all = de.get_train_test_all(con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Darts TimeSeries objects for model training/prediction\n",
    "all_series = de.get_series(lmp_all)\n",
    "train_test_all_series = de.get_series(train_test_all)\n",
    "train_series = de.get_series(train_all)\n",
    "test_series = de.get_series(test_all)\n",
    "\n",
    "# Future covariates (known ahead of time, e.g. weather/load forecasts)\n",
    "# and past covariates (only known up to current time)\n",
    "futr_cov = de.get_futr_cov(all_df_pd)\n",
    "past_cov = de.get_past_cov(all_df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Train TOP_N TSMixer models with different hyperparameter sets\n",
    "    models_tsmixer = []\n",
    "    if parameters.USE_TSMIXER:\n",
    "        for i, param in enumerate(parameters.TSMIXER_PARAMS[: parameters.TOP_N]):\n",
    "            print(f\"\\ni: {i} \\t\" + \"*\" * 25, flush=True)\n",
    "            model_tsmixer = build_fit_tsmixerx(\n",
    "                series=train_test_all_series,\n",
    "                val_series=test_series,\n",
    "                future_covariates=futr_cov,\n",
    "                past_covariates=past_cov,\n",
    "                **param,\n",
    "            )\n",
    "            models_tsmixer += [model_tsmixer]\n",
    "\n",
    "    return models_tsmixer \n",
    "\n",
    "models_tsmixer = _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Train TOP_N TiDE models with different hyperparameter sets\n",
    "    models_tide = []\n",
    "    if parameters.USE_TIDE:\n",
    "        for i, param in enumerate(parameters.TIDE_PARAMS[: parameters.TOP_N]):\n",
    "            print(f\"\\ni: {i} \\t\" + \"*\" * 25, flush=True)\n",
    "            model_tide = build_fit_tide(\n",
    "                series=train_test_all_series,\n",
    "                val_series=test_series,\n",
    "                future_covariates=futr_cov,\n",
    "                past_covariates=past_cov,\n",
    "                **param,\n",
    "            )\n",
    "            models_tide += [model_tide]\n",
    "\n",
    "    return models_tide\n",
    "\n",
    "\n",
    "models_tide = _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Train TOP_N TFT models with different hyperparameter sets\n",
    "    models_tft = []\n",
    "    if parameters.USE_TFT:\n",
    "        for i, param in enumerate(parameters.TFT_PARAMS[: parameters.TOP_N]):\n",
    "            print(f\"\\ni: {i} \\t\" + \"*\" * 25, flush=True)\n",
    "            model_tft = build_fit_tft(\n",
    "                series=train_test_all_series,\n",
    "                val_series=test_series,\n",
    "                future_covariates=futr_cov,\n",
    "                past_covariates=past_cov,\n",
    "                **param,\n",
    "            )\n",
    "            models_tft += [model_tft]\n",
    "\n",
    "    return models_tft\n",
    "\n",
    "models_tft = _()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "## Save and upload models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamped folder path for this retrain run's artifacts.\n",
    "# artifact_folder: relative path passed to utils.get_loaded_models()\n",
    "# artifact_path:   full S3 key prefix for uploading files\n",
    "utc_timestamp = pd.Timestamp.utcnow()\n",
    "log.info(f'{utc_timestamp = }')\n",
    "\n",
    "folder_time = utc_timestamp.strftime('%Y-%m-%d_%H-%M-%S') + '/'\n",
    "log.info(f'{folder_time = }')\n",
    "\n",
    "artifact_folder = 'model_retrains/' + folder_time\n",
    "log.info(f'{artifact_folder = }')\n",
    "\n",
    "artifact_path = AWS_S3_FOLDER + artifact_folder\n",
    "log.info(f'{artifact_path = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Upload all trained models to S3 under the timestamped artifact path.\n",
    "    # Darts' .save() produces two files per model:\n",
    "    #   <name>.pt      - pickled model wrapper (config, training state)\n",
    "    #   <name>.pt.ckpt - PyTorch Lightning checkpoint (neural network weights)\n",
    "    # Both are required for Darts' .load() to fully restore a trained model.\n",
    "\n",
    "    upload_paths = []\n",
    "\n",
    "    def model_to_tmp_upload(\n",
    "        m,\n",
    "        name: str,\n",
    "        AWS_S3_BUCKET: str=AWS_S3_BUCKET,\n",
    "        artifact_path: str=artifact_path,\n",
    "    ):\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            model_path = os.path.join(tmpdir, name)\n",
    "            m.save(model_path)\n",
    "\n",
    "            # Upload the model wrapper file\n",
    "            upload_path = artifact_path + name\n",
    "            s3.upload_file(model_path, AWS_S3_BUCKET, upload_path)\n",
    "            log.info(f'Uploaded: {upload_path}')\n",
    "\n",
    "            # Upload the checkpoint file with neural network weights\n",
    "            ckpt_path = model_path + '.ckpt'\n",
    "            if os.path.exists(ckpt_path):\n",
    "                ckpt_upload_path = upload_path + '.ckpt'\n",
    "                s3.upload_file(ckpt_path, AWS_S3_BUCKET, ckpt_upload_path)\n",
    "                log.info(f'Uploaded: {ckpt_upload_path}')\n",
    "\n",
    "        return upload_path\n",
    "\n",
    "    # Upload training timestamp so the Shiny app can display when models were last trained\n",
    "    buffer = io.BytesIO()\n",
    "    pickle.dump(utc_timestamp, buffer)\n",
    "    buffer.seek(0)\n",
    "    upload_path = artifact_path + \"TRAIN_TIMESTAMP.pkl\"\n",
    "    s3.put_object(\n",
    "        Bucket=AWS_S3_BUCKET, \n",
    "        Key=upload_path, \n",
    "        Body=buffer,\n",
    "    )\n",
    "    log.info(f'Uploaded: {upload_path}')\n",
    "    upload_paths+=[upload_path]\n",
    "\n",
    "    for i, m in enumerate(models_tide):\n",
    "        upload_paths+=[model_to_tmp_upload(m, f\"tide_{i}.pt\")]\n",
    "\n",
    "    for i, m in enumerate(models_tsmixer):\n",
    "        upload_paths+=[model_to_tmp_upload(m, f\"tsmixer_{i}.pt\")]\n",
    "\n",
    "    for i, m in enumerate(models_tft):\n",
    "        upload_paths+=[model_to_tmp_upload(m, f\"tft_{i}.pt\")]\n",
    "\n",
    "    return upload_paths\n",
    "\n",
    "upload_paths = _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model files just uploaded to S3 for verification and loading\n",
    "loaded_models_for_test = utils.get_loaded_models(artifact_folder)\n",
    "loaded_models_for_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "## Test loading models from S3 and doing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DnEU",
   "metadata": {},
   "source": [
    "### Load models by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Re-download models from S3 and load them to verify the save/load\n",
    "    # round-trip works before promoting to champion.\n",
    "    # Each model needs both .pt and .pt.ckpt files in the same directory\n",
    "    # for Darts' .load() to fully restore the trained model.\n",
    "\n",
    "    def get_checkpoints(\n",
    "        model_filter: str, # 'tsmixer_', 'tide_', or 'tft_'\n",
    "        loaded_models_for_test: List[str]=loaded_models_for_test,\n",
    "    ):\n",
    "        \"\"\"Filter S3 keys to just the main .pt files (not .ckpt or .pkl).\"\"\"\n",
    "        return [\n",
    "            f for f in loaded_models_for_test\n",
    "            if model_filter in f and \".pt\" in f and \".ckpt\" not in f\n",
    "            and \"TRAIN_TIMESTAMP.pkl\" not in f\n",
    "        ]\n",
    "\n",
    "    def load_model_from_s3(model_class, key):\n",
    "        \"\"\"Download a model's .pt + .pt.ckpt files to a temp dir and load.\"\"\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            filename = key.split('/')[-1]\n",
    "            local_path = os.path.join(tmpdir, filename)\n",
    "\n",
    "            # Download the model wrapper file\n",
    "            s3.download_file(Bucket=AWS_S3_BUCKET, Key=key, Filename=local_path)\n",
    "\n",
    "            # Download the checkpoint file with neural network weights\n",
    "            try:\n",
    "                s3.download_file(\n",
    "                    Bucket=AWS_S3_BUCKET,\n",
    "                    Key=key + '.ckpt',\n",
    "                    Filename=local_path + '.ckpt',\n",
    "                )\n",
    "            except Exception:\n",
    "                log.warning(f\"No checkpoint file found for {key}\")\n",
    "\n",
    "            log.info(f\"loading model: {key}\")\n",
    "            model = model_class.load(local_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    ts_mixer_ckpts = get_checkpoints(\"tsmixer_\")\n",
    "    ts_mixer_forecasting_models = [load_model_from_s3(TSMixerModel, m) for m in ts_mixer_ckpts]\n",
    "\n",
    "    tide_ckpts = get_checkpoints(\"tide_\")\n",
    "    tide_forecasting_models = [load_model_from_s3(TiDEModel, m) for m in tide_ckpts]\n",
    "\n",
    "    tft_ckpts = get_checkpoints(\"tft_\")\n",
    "    tft_forecasting_models = [load_model_from_s3(TFTModel, m) for m in tft_ckpts]\n",
    "\n",
    "    forecasting_models = (\n",
    "        ts_mixer_forecasting_models\n",
    "        + tide_forecasting_models\n",
    "        + tft_forecasting_models\n",
    "    )\n",
    "\n",
    "    return forecasting_models\n",
    "\n",
    "forecasting_models = _()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {},
   "source": [
    "## Create ensemble model and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Combine all reloaded models into a NaiveEnsembleModel (simple average\n",
    "    # of predictions). train_forecasting_models=False since they're already trained.\n",
    "    log.info(\"loading model from checkpoints\")\n",
    "    loaded_model = NaiveEnsembleModel(\n",
    "        forecasting_models=forecasting_models,\n",
    "        train_forecasting_models=False,\n",
    "    )\n",
    "\n",
    "    # Smoke test: run a short prediction on one node to verify the\n",
    "    # ensemble works end-to-end before promoting to champion.\n",
    "    log.info(\"test getting predictions\")\n",
    "    plot_ind = 3\n",
    "    plot_series = all_series[plot_ind]\n",
    "\n",
    "    plot_end_time = plot_series.end_time() - pd.Timedelta(\n",
    "        f\"{parameters.INPUT_CHUNK_LENGTH + 1}h\"\n",
    "    )\n",
    "    log.info(f\"plot_end_time: {plot_end_time}\")\n",
    "\n",
    "    plot_node_name = plot_series.static_covariates.unique_id.LMP\n",
    "    node_series = plot_series.drop_after(plot_end_time)\n",
    "    log.info(f\"plot_end_time: {plot_end_time}\")\n",
    "    log.info(f\"node_series.end_time(): {node_series.end_time()}\")\n",
    "    future_cov_series = futr_cov[0]\n",
    "    past_cov_series = past_cov[0]\n",
    "\n",
    "    pred = loaded_model.predict(\n",
    "        series=node_series,\n",
    "        past_covariates=past_cov_series,\n",
    "        future_covariates=future_cov_series,\n",
    "        n=5,\n",
    "        num_samples=2,\n",
    "    )\n",
    "\n",
    "    log.info(f\"pred: {pred}\")\n",
    "\n",
    "    return pred\n",
    "\n",
    "pred = _()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.pd_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _():\n",
    "    # Upload champion.json to S3_models/ so the Shiny app knows which\n",
    "    # retrain folder contains the current production models.\n",
    "    # Only promoted if the smoke-test prediction succeeded.\n",
    "    if pred is not None:\n",
    "        champion_json = {\n",
    "            \"champion\": folder_time,\n",
    "            \"champion_artifact_folder\": artifact_folder,\n",
    "            \"champion_artifact_path\": artifact_path,\n",
    "        }\n",
    "\n",
    "        buffer = io.BytesIO(json.dumps(champion_json).encode(\"utf-8\"))\n",
    "        champion_key = AWS_S3_FOLDER + \"S3_models/champion.json\"\n",
    "        s3.put_object(Bucket=AWS_S3_BUCKET, Key=champion_key, Body=buffer)\n",
    "        log.info(f\"Uploaded champion model json: {champion_key}\")\n",
    "        log.info(f\"champion_json: {champion_json}\")\n",
    "    else:\n",
    "        log.warning(\"Prediction failed, not saving json\")\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time as _time\n",
    "\n",
    "t1 = time()\n",
    "log.info(\"finished retraining\")\n",
    "log.info(f\"total time (min): {(t1 - t0) / 60:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "marimo": {
   "app_config": {
    "width": "medium"
   },
   "header": "# Model retraining notebook for SPP WEIS nodal price forecasting.\n#\n# Workflow:\n#   1. Connect to S3-backed database and prepare LMP + covariate data\n#   2. Train TSMixer, TiDE, and TFT models using top-N hyperparameter sets\n#   3. Save trained models to S3 using Darts' native serialization (.pt + .pt.ckpt)\n#   4. Reload models from S3 and verify predictions via a NaiveEnsembleModel\n#   5. Upload champion.json so the Shiny app knows which model folder to load\n\n",
   "marimo_version": "0.19.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
